<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>How to migrate a complex JBoss EAP application to OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/23/how-migrate-complex-jboss-eap-application-openshift" /><author><name>Philip Hayes</name></author><id>e3e03ffe-d779-4cc9-a6f7-dc2b76ccd0c5</id><updated>2023-08-23T07:00:00Z</updated><published>2023-08-23T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; is an ideal location to re-platform &lt;a href="https://developers.redhat.com/products/eap/overview"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; (JBoss EAP) workloads. We discussed this topic in previous articles, such as &lt;a href="https://developers.redhat.com/articles/2022/01/07/why-you-should-migrate-your-java-workloads-openshift"&gt;Why you should migrate your Java workloads to OpenShift.&lt;/a&gt; Here's a summary of the benefits:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: OpenShift can automatically scale JBoss EAP instances based on demand, making it more efficient and responsive to changes in traffic.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Immutability:&lt;/strong&gt; JBoss EAP applications deployed on OpenShift are built as immutable images, making it easier to promote with confidence from lower to higher environments and removing the possibility of configuration creep.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Resource allocation&lt;/strong&gt;: On OpenShift, resources can be allocated dynamically based on actual usage, allowing for better resource utilization and cost efficiency.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Deployment process&lt;/strong&gt;: OpenShift provides automated deployment and management of JBoss EAP instances, reducing the risk of errors and simplifying the deployment process.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;High availability&lt;/strong&gt;: In OpenShift, high availability is built in with automatic failover and load balancing across JBoss EAP instances.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;DevOps integration&lt;/strong&gt;: OpenShift provides a &lt;a href="https://developers.redhat.com/articles/2023/08/07/how-fully-utilize-openshift-devops"&gt;DevOps-friendly platform&lt;/a&gt; that supports &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;continuous integration and continuous deployment (CI/CD)&lt;/a&gt; workflows, making it easier to integrate JBoss EAP applications into a larger DevOps pipeline.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In the article, &lt;a href="https://developers.redhat.com/articles/2022/01/12/how-migrate-your-java-applications-red-hat-openshift"&gt;How to migrate your Java applications to Red Hat OpenShift&lt;/a&gt;, we went through the steps to migrate a simple JBoss EAP application. But what's involved with deploying a more complex real-world JBoss EAP application to OpenShift? We will describe the steps to deploy a monolith JBoss EAP application using message queues and clustering. For the purpose of this exercise, we'll use the &lt;a href="https://github.com/deewhyweb/eap-coolstore-monolith/tree/ocp"&gt;CoolStore Monolith&lt;/a&gt; application.&lt;/p&gt; &lt;p&gt;The main points of this application are as follows:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;It uses message driven beans that rely on an embedded message queue for communication.&lt;/li&gt; &lt;li aria-level="1"&gt;It uses single sign-on (SSO) for authorization.&lt;/li&gt; &lt;li aria-level="1"&gt;It requires a PostgreSQL database for persistence.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;If we look at a &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.4/html-single/getting_started_with_jboss_eap_for_openshift_container_platform/index#comparison_eap_and_xpaas_eap_image"&gt;comparison of JBoss EAP on a VM and JBoss EAP on OpenShift&lt;/a&gt;, we can see embedded messaging is not supported. Because our application requires a message queue, we need to deploy a separate instance of AMQ broker.&lt;/p&gt; &lt;p&gt;In addition to deploying our application, we will also need to deploy a PostgreSQL database, AMQ broker, and SSO.&lt;/p&gt; &lt;h2&gt;Single sign-on&lt;/h2&gt; &lt;p&gt;To deploy single sign-on in OpenShift, we'll use the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.6/html/server_installation_and_configuration_guide/operator"&gt;SSO operator&lt;/a&gt;. This operator is comprehensive, allowing us to not only install an SSO cluster but also configure the required realm, client, and user objects.&lt;/p&gt; &lt;p&gt;Our sample application must have a realm, client, and user created. In addition to this, our application also needs to know the URL for the SSO instance. This URL can be configured as an environment variable when the application is deployed.&lt;/p&gt; &lt;h2&gt;AMQ broker&lt;/h2&gt; &lt;p&gt;As we mentioned earlier, embedded messaging is not supported when JBoss EAP is deployed on OpenShift. The recommended approach is to use an instance of AMQ broker to provide messaging functionality.&lt;/p&gt; &lt;p&gt;As with SSO, we can use the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.4/html/deploying_amq_broker_on_openshift_container_platform/broker-operator-broker-ocp"&gt;AMQ broker operator&lt;/a&gt; to deploy an AMQ broker instance on OpenShift. Once the operator is deployed we can use custom resources to create an AMQ broker cluster and the required queues and topics. To create the AMQ broker cluster we can use the following custom resource:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: broker.amq.io/v1beta1 kind: ActiveMQArtemis metadata:   name: eap74-amq7 spec:   acceptors:     - name: my-acceptor       port: 61616       protocols: 'core'   deploymentPlan:     image: placeholder     jolokiaAgentEnabled: false     journalType: nio     managementRBACEnabled: true     messageMigration: false     persistenceEnabled: false     requireLogin: false     size: 2 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once this cluster is created, we need to create the topic required by the application.&lt;/p&gt; &lt;p&gt;Looking at our app source code, we can see our application needs a topic named &lt;strong&gt;topic/orders&lt;/strong&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;@ActivationConfigProperty(propertyName = "destinationLookup", propertyValue = "topic/orders")&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To create this topic in our AMQ broker instance, we can use the following custom resource:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: broker.amq.io/v1beta1 kind: ActiveMQArtemisAddress metadata:   name: artemis-address-topic spec:   addressName: topic.orders   queueName: topic/orders   routingType: multicast&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We set the &lt;code&gt;routingType&lt;/code&gt; to &lt;code&gt;multicast&lt;/code&gt; because our application requires sending messages to every consumer subscribed to an address. This is described in the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.0/html/using_amq_broker/addresses#configuring_publish_subscribe_messaging"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;PostgreSQL database&lt;/h2&gt; &lt;p&gt;There are multiple ways to deploy PostgreSQL databases on OpenShift. Here, we created a &lt;a href="https://github.com/deewhyweb/eap-coolstore-monolith/blob/ocp/openshift/resources/psql.yml"&gt;simple YAML script&lt;/a&gt; to create the deployment, service, and secret required to deploy the database.&lt;/p&gt; &lt;h2&gt;Application analysis&lt;/h2&gt; &lt;p&gt;During the image building process, a JBoss EAP instance is provisioned to host our application. When this JBoss EAP instance is provisioned, we can determine which layers to use to ensure our application has the capabilities required to run and remove any unnecessary capabilities to improve resource usage and reduce the attack surface.&lt;/p&gt; &lt;p&gt;You should review the supported &lt;a href="https://access.redhat.com/documentation/fr-fr/red_hat_jboss_enterprise_application_platform/7.4/html/getting_started_with_jboss_eap_for_openshift_container_platform/capability-trimming-eap-foropenshift_default#available-jboss-eap-layers_default"&gt;JBoss EAP layers&lt;/a&gt; and &lt;a href="https://docs.wildfly.org/21/Galleon_Guide.html#wildfly_galleon_layers"&gt;WildFly layers&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Looking at our application code, we can determine the following:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;The application uses a PostgreSQL database, so it will need the PostgreSQL JDBC driver.&lt;/li&gt; &lt;li aria-level="1"&gt;The application requires web clustering, due to the presence of &lt;distributable&gt; in &lt;code&gt;web.xml&lt;/code&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;The application uses a JMS queue.&lt;/li&gt; &lt;li aria-level="1"&gt;The application includes the javax.ejb package, so EJB is required.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We can address a few of these requirements from the set of layers included with JBoss EAP 7.4:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;The &lt;strong&gt;cloud-server&lt;/strong&gt; layer is the minimal layer for JBoss EAP running on OpenShift. This layer includes the health and metrics subsystems, and also includes messaging-activemq. So this meets our requirement for message queue connectivity.&lt;/li&gt; &lt;li aria-level="1"&gt;We can use the &lt;strong&gt;web-clustering&lt;/strong&gt; layer for clustering.&lt;/li&gt; &lt;li aria-level="1"&gt;We can use the &lt;strong&gt;ejb&lt;/strong&gt; layer (from the list of WildFly layers) for ejb support.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;This leaves support for the PostgreSQL database. To add this, we can use the &lt;a href="https://github.com/jbossas/eap-datasources-galleon-pack"&gt;JBoss EAP datasources feature pack&lt;/a&gt;. This feature pack adds a postgresql-datasource layer that installs the required driver for PostgreSQL and sets up a JDBC DataSource to connect to the database.&lt;/p&gt; &lt;p&gt;All we need to do is define the version of the PostgreSQL JDBC driver as an environment variable: &lt;code&gt;POSTGRESQL_DRIVER_VERSION&lt;/code&gt;. We will use version 42.6.0.&lt;/p&gt; &lt;p&gt;To add these layers, environment variables, and feature packs to our application, we can specify the following to our Helm chart configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;build: uri: https://github.com/deewhyweb/eap-coolstore-monolith.git ref: ocp s2i: featurePacks: - 'org.jboss.eap:eap-datasources-galleon-pack:7.4.0.GA-redhat-00003' galleonLayers: - cloud-server - postgresql-datasource - ejb - web-clustering env: - name: POSTGRESQL_DRIVER_VERSION value: 42.6.0 deploy: enabled: false&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Application configuration&lt;/h2&gt; &lt;p&gt;Once our application is deployed to OpenShift, we will configure it to connect to our instances of PostgreSQL, AMQ broker, and SSO. All configuration is done by providing environment variables either through a ConfigMap or inline on the WildFly Server custom resource definition.&lt;/p&gt; &lt;p&gt;When we deployed our AMQ broker with the ActiveMQArtemis custom resource definition, we named the instance eap74-amq7. We must use this name in the &lt;code&gt;MQ_SERVICE_PREFIX_MAPPING&lt;/code&gt;, &lt;code&gt;EAP74_AMQ_TCP_SERVICE_HOST&lt;/code&gt;, and &lt;code&gt;AMQ_JNDI&lt;/code&gt;, as shown in the following snippet. This is documented in the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.4/html-single/getting_started_with_jboss_eap_for_openshift_container_platform/index#configuring_external_red_hat_amq_brokers"&gt;Getting Started with JBoss EAP for OpenShift Container Platform&lt;/a&gt; guide.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;  MQ_SERVICE_PREFIX_MAPPING: eap74-amq7=MQ   EAP74_AMQ_TCP_SERVICE_HOST: eap74-amq7-hdls-svc   EAP74_AMQ_TCP_SERVICE_PORT: "61616"   MQ_TOPICS: orders   AMQ_JNDI: java:/eap74-amq7/ConnectionFactory&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To pull credentials from the &lt;code&gt;eap74-amq7-credentials-secret&lt;/code&gt; secret, use these configurations:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;    - name: MQ_USERNAME       valueFrom:         secretKeyRef:           key: AMQ_USER           name: eap74-amq7-credentials-secret     - name: MQ_PASSWORD       valueFrom:         secretKeyRef:           key: AMQ_PASSWORD           name: eap74-amq7-credentials-secret&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To connect to the PostgreSQL database, add the following environment variables:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;  POSTGRESQL_DATABASE: postgresDB   POSTGRESQL_DATASOURCE: CoolstoreDS   POSTGRESQL_SERVICE_HOST: postgresql&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To pull credentials from the PostgreSQL secret, add these configurations:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;    - name: POSTGRESQL_PASSWORD       valueFrom:         secretKeyRef:           key: database-password           name: postgresql     - name: POSTGRESQL_USER       valueFrom:         secretKeyRef:           key: database-user           name: postgresql&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To connect to the SSO instance, add the following environment variable: &lt;code&gt;KEYCLOAK_URL&lt;/code&gt;. We can determine this URL once single sign-on is deployed.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;KEYCLOAK_URL: https:/&lt;&lt;red hat sso url&gt;&gt;/auth&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Clustering support&lt;/h2&gt; &lt;p&gt;Because our application requires clustering support, we must ensure our pods are deployed using a service account with permissions to view the pods in the same namespace. The default service account does not have this permission.&lt;/p&gt; &lt;p&gt;Create a service account called &lt;code&gt;coolstoresa&lt;/code&gt;. Then create a role called &lt;code&gt;pod-viewer&lt;/code&gt; and assign this role to the coolstoresa service account. We will accomplish this by applying the following YAML:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: v1 kind: ServiceAccount metadata: name: coolstoresa --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: pod-viewer rules: - apiGroups: [""] resources: ["pods"] verbs: ["get", "watch", "list"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: pod-viewer roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: pod-viewer subjects: - kind: ServiceAccount name: coolstoresa&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the pod starts up, you should see the following log, indicating clustering is configured correctly:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;INFO Service account has sufficient permissions to view pods in kubernetes (HTTP 200). Clustering will be available.&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Testing the application by deploying on OpenShift&lt;/h2&gt; &lt;p&gt;In this section, we're going to explain the steps to deploy this application and all required components on OpenShift.&lt;/p&gt; &lt;h3&gt;Prerequisites&lt;/h3&gt; &lt;p&gt;Before we begin, you will need to complete the following:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;OpenShift 4.12 with cluster admin access.&lt;/li&gt; &lt;li aria-level="1"&gt;OpenShift CLI logged into cluster with cluster admin access.&lt;/li&gt; &lt;li aria-level="1"&gt;Check out the code from &lt;a href="https://github.com/deewhyweb/eap-coolstore-monolith.git"&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Run the following commands from the local directory into where the code has been cloned. Make sure to check out the ocp branch first with:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; git checkout ocp&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 1: Create a project&lt;/h3&gt; &lt;p&gt;Create a new project in OpenShift by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc new-project coolstore&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will create a new project called coolstore.&lt;/p&gt; &lt;h3&gt;Step 2: Deploy the operators&lt;/h3&gt; &lt;p&gt;Deploy the SSO, AMQ broker, and JBoss EAP operators by creating subscription custom resources.&lt;/p&gt; &lt;p&gt;Run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc apply -f./openshift/operators&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will deploy the necessary operators to the cluster. Wait for the operators to be deployed by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get csv -w&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Wait until their ClusterServiceVersion PHASE is set to "Succeeded".&lt;/p&gt; &lt;h3&gt;Step 3: Deploy resources&lt;/h3&gt; &lt;p&gt;Now that the operators have been deployed, we can create the coolstoresa service account and deploy the SSO, AMQ broker, and PostgreSQL instances.&lt;/p&gt; &lt;p&gt;Run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc apply -f./openshift/resources&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then run the following command to get the route for the SSO instance:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get route keycloak&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It may take a few attempts to create the route. Once you have the route, update the &lt;code&gt;KEYCLOAK_URL&lt;/code&gt; value in &lt;code&gt;helm.yaml&lt;/code&gt; with the correct route for SSO. Make sure to prepend &lt;code&gt;https://&lt;/code&gt; and append &lt;code&gt;/auth&lt;/code&gt; to this URL. The value should look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;  - name: KEYCLOAK_URL     value: https://keycloak-coolstore.apps.openshift.domainname.com/auth&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 4: Build the application image&lt;/h3&gt; &lt;p&gt;To build the application image, navigate to the developer UI and click on &lt;strong&gt;+Add&lt;/strong&gt;, then &lt;strong&gt;Helm Chart.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Select the Eap74 Helm chart.&lt;/p&gt; &lt;p&gt;Paste the contents of &lt;code&gt;openshift/helm.yml&lt;/code&gt; as the config.&lt;/p&gt; &lt;p&gt;Wait for the application image to be built by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get build -w&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The application image is built when the eap74-2 build is complete.&lt;/p&gt; &lt;h3&gt;Step 5: Deploy the application&lt;/h3&gt; &lt;p&gt;We can deploy the application by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc apply -f./openshift/app&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 6: Testing the application&lt;/h3&gt; &lt;p&gt;Once the application is running, you should see a topology similar to Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/topology_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/topology_0.jpg?itok=sSOvEE7M" width="600" height="389" alt="A screenshot of the OpenShift topology view, showing running applications." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The OpenShift topology view of the deployed application, showing running applications.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;You should be able to access this via the external route. From the application, click on &lt;strong&gt;Sign in&lt;/strong&gt; in the top right-hand corner. This will take you to the single sign-on login page. Log in with the credentials user1 / pass.&lt;/p&gt; &lt;p&gt;Now, you should be able to add products to your cart and complete the checkout process.&lt;/p&gt; &lt;h3&gt;Step 7: Testing clustering&lt;/h3&gt; &lt;p&gt;If we scale up our application, we should see additional members forming a cluster. To scale up the application, run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc scale WildFlyServer eap74 --replicas=2&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;While the second instance starts, monitor the logs of the existing pod. You should see the addition of a node to the cluster similar to the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;10:46:45,408 INFO [org.infinispan.CLUSTER] (ServerService Thread Pool -- 63) ISPN000078: Starting JGroups channel ee 10:46:45,501 INFO [org.infinispan.CLUSTER] (ServerService Thread Pool -- 63) ISPN000094: Received new cluster view for channel ee: [eap74-0] (2) [eap74-0, eap74-1] 10:46:45,507 INFO [org.infinispan.CLUSTER] (ServerService Thread Pool -- 63) ISPN000079: Channel ee local address is eap74-1, physical addresses are [10.130.0.67:7600]&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;JBoss EAP migrated to OpenShift&lt;/h2&gt; &lt;p&gt;In this article, we migrated a real-world monolith JBoss EAP application to OpenShift. The application required a PostgreSQL database, message queue, and single sign-on. We used &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; operators to deploy and configure AMQ broker and single sign-on.&lt;/p&gt; &lt;p&gt;We also performed analysis on the application to determine which layers to use and concluded the requirement of the following layers:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;cloud-server&lt;/li&gt; &lt;li&gt;ejb&lt;/li&gt; &lt;li&gt;web-clustering&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In addition to these layers, we also used the postgresql-datasource from the JBoss EAP-datasources-galleon-pack to provide PostgreSQL database drivers and configuration.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/23/how-migrate-complex-jboss-eap-application-openshift" title="How to migrate a complex JBoss EAP application to OpenShift"&gt;How to migrate a complex JBoss EAP application to OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Philip Hayes</dc:creator><dc:date>2023-08-23T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 3.3.0 released - OpenTelemetry improvements, Reactive Messaging Pulsar extension</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-3-3-0-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-3-3-0-released/</id><updated>2023-08-23T00:00:00Z</updated><content type="html">It is our pleasure to announce the release of Quarkus 3.3.0. The first thing you will notice in this release is that we dropped the .Final suffix. This suffix was introduced to make sure final releases were sorted properly compared to alphas, beta and candidate releases, at a time where...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title>Migration toolkit for applications 6.2: Agile Java modernization</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/22/migration-toolkit-applications-62-agile-java-modernization" /><author><name>Yashwanth Maheshwaram</name></author><id>a1f6500c-f583-4d1c-8cc7-9e2255a829fa</id><updated>2023-08-22T17:30:00Z</updated><published>2023-08-22T17:30:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/mta/overview"&gt;Red Hat's migration toolkit for applications 6.2&lt;/a&gt; takes &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; application modernization a step further with new integrations for Jira, migration waves management, and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; monitoring.&lt;/p&gt; &lt;p&gt;With these integrations, the migration toolkit for applications can now help organizations create a backlog of work directly on Jira so that your engineering teams can pick up tickets. The support for migration waves enables you to process the backlog in batches and then plan for the whole application modernization at once. You can also make migration waves available directly on Jira. &lt;/p&gt; &lt;p&gt;Read on for a complete list of new features in the migration toolkit for applications 6.2, available with a Red Hat OpenShift subscription.&lt;/p&gt; &lt;h2&gt;Jira integration&lt;/h2&gt; &lt;p&gt;The integration of the migration toolkit for applications with Jira allows you to track and manage the whole migration process. To introduce changes to the applications in the portfolio, you can create issues in Jira and assign them to developers.&lt;/p&gt; &lt;p&gt;For more information, see &lt;a href="https://access.redhat.com/documentation/en-us/migration_toolkit_for_applications/6.2/html/user_interface_guide/creating-configuring-jira-connection#mta-web-create-config-jira-connection_user-interface-guide"&gt;Creating and configuring a Jira connection&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Migration waves management&lt;/h2&gt; &lt;p&gt;A migration wave is a small collection of workloads that deliver business value. Migration waves group applications to be migrated on a specified schedule.&lt;/p&gt; &lt;p&gt;In addition, a migration wave enables you to export a list of the wave’s applications to the Jira issue management system. This automatically creates a separate Jira issue for each application of the migration wave for tracking.&lt;/p&gt; &lt;p&gt;For more information, see &lt;a href="https://access.redhat.com/documentation/en-us/migration_toolkit_for_applications/6.2/html/user_interface_guide/working-with-applications-in-the-ui#mta-web-creating-migration-waves_user-interface-guide"&gt;Creating migration waves&lt;/a&gt; and &lt;a href="https://access.redhat.com/documentation/en-us/migration_toolkit_for_applications/6.2/html/user_interface_guide/working-with-applications-in-the-ui#mta-web-creating-jira-issues-for-migration-wave_user-interface-guide"&gt;Creating Jira issues for a migration wave&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;OpenShift monitoring integration&lt;/h2&gt; &lt;p&gt;The migration toolkit for applications integrates with Red Hat OpenShift's monitoring stack. This allows Red Hat to collect data that will display the usage of the migration toolkit in the field by customers beyond installation.&lt;/p&gt; &lt;h2&gt;Modernize securely at scale&lt;/h2&gt; &lt;p&gt;Red Hat's migration toolkit for applications provides a simpler, faster way for development teams modernize and migrate applications to Red Hat OpenShift, providing tools and best practices to accelerate your journey to Kubernetes. You can get access to the migration toolkit for applications with an OpenShift subscription. &lt;a href="https://developers.redhat.com/products/mta/getting-started"&gt;Get started today.&lt;/a&gt;&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/22/migration-toolkit-applications-62-agile-java-modernization" title="Migration toolkit for applications 6.2: Agile Java modernization"&gt;Migration toolkit for applications 6.2: Agile Java modernization&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Yashwanth Maheshwaram</dc:creator><dc:date>2023-08-22T17:30:00Z</dc:date></entry><entry><title type="html">How to create Jobs in Kubernetes</title><link rel="alternate" href="https://www.mastertheboss.com/soa-cloud/openshift/how-to-create-jobs-in-kubernetes/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/soa-cloud/openshift/how-to-create-jobs-in-kubernetes/</id><updated>2023-08-22T12:52:25Z</updated><content type="html">This article discusses how to automate Tasks in Kubernetes and OpenShift using Jobs and Cron Jobs. We will show some example on how to create and manage them. Then, we will discuss the best practices about using Jobs in Kubernetes. In a Kubernetes environment, you can use Jobs to automate tasks that need to run ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Enhance Ansible development experience with Lightspeed</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/22/enhance-ansible-development-experience-lightspeed" /><author><name>Nagesh Rathod</name></author><id>cf8a9c07-f235-4911-813c-973e23674812</id><updated>2023-08-22T07:00:00Z</updated><published>2023-08-22T07:00:00Z</published><summary type="html">&lt;p&gt;Ansible Lightspeed is a generative AI tool that provides an efficient way for developers to create &lt;a href="https://developers.redhat.com/products/ansible/"&gt;Ansible&lt;/a&gt; content and &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; tasks for Ansible playbooks. In this article, we will explore how to install and use Ansible Lightspeed in Visual Studio Code. This makes it easy to create Ansible content; you can simply type in plain English descriptions and Ansible Lightspeed will generate the code recommendations for you.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;Before proceeding through this tutorial, make sure you have installed the following:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Visual Studio Code (VS Code)&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; 3.9+&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/products/ansible/"&gt;Ansible Automation Platform&lt;/a&gt; 2.x (see &lt;a href="https://developers.redhat.com/blog/2023/08/04/how-install-ansible-automation-platform-24-rhel-91"&gt;How to install Ansible Automation Platform 2.4 on RHEL 9.1&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;1. Install Ansible extension in VS Code&lt;/h2&gt; &lt;p&gt;The Ansible extension offers intelligent code completion, syntax highlighting, and error checking for YAML and Ansible-specific files. The extension allows users to execute Ansible tasks and playbooks directly from the editor and view the real-time output. With built-in support for Ansible Vault and Galaxy, developers can efficiently manage secrets and access a vast library of community-contributed roles.&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt; &lt;p&gt;In VS Code, click the extension icon in the left menu. In the search field, type &lt;code&gt;Ansible&lt;/code&gt; to bring up the Ansible extension by Red Hat.&lt;/p&gt; &lt;/li&gt; &lt;li aria-level="1"&gt;Click the &lt;strong&gt;Install &lt;/strong&gt;button. &lt;/li&gt; &lt;li aria-level="1"&gt;After installing the Ansible extension in the integrated development environment (IDE), click on the little gear icon beside the Install button and select the &lt;strong&gt;Extension Setting &lt;/strong&gt;option, as illustrated in Figure 1.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture1.png?itok=hdan4ffL" width="600" height="351" alt="Vs code extention" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Installing the Ansible extension in Visual Studio Code.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;2. Enable Ansible Lightspeed in VS Code&lt;/h2&gt; &lt;p&gt;You will get the following settings page of Ansible Extension, from which you can make changes to execution environment image, container registry, Ansible lint, and many more. This article only covers Lightspeed, so we will just enable the &lt;strong&gt;Ansible Lightspeed&lt;/strong&gt; option and&lt;strong&gt; Ansible Lightspeed with Watson Code Assistant inline suggestions&lt;/strong&gt;. Please refer to Figure 2 to set the configuration.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture2.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture2.png?itok=zFCHJwNZ" width="600" height="338" alt="enable it" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Enable Lightspeed and Watson Code Assistant.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you get the error "Ansible-lint is missing," use the following commands to install it. There are two methods to install: &lt;code&gt;pip&lt;/code&gt; and &lt;code&gt;dnf&lt;/code&gt;.&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Install using &lt;code&gt;pip&lt;/code&gt;: &lt;pre&gt; &lt;code class="language-bash"&gt;pip3 install ansible-lint&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;ol start="2"&gt;&lt;li aria-level="1"&gt;Install with the &lt;code&gt;dnf&lt;/code&gt; package manager: &lt;pre&gt; &lt;code class="language-bash"&gt;dnf install ansible-lint&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;3. Authenticate Lightspeed with GitHub&lt;/h2&gt; &lt;p&gt;Click on the Ansible extension icon on the left bar. You will see the &lt;strong&gt;Connect&lt;/strong&gt; button, as shown in Figure 3.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture3.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture3.png?itok=Wmb7FBir" width="600" height="690" alt="connect" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Connect Ansible to authenticate.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;It will redirect to your default browser and open the GitHub login page. &lt;/p&gt; &lt;p&gt;Ansible Lightspeed requires GitHub authentication. Sign in using your GitHub credentials and follow the prompts. Once done, you will be redirected to VS Code and the Connect button should now be replaced with your GitHub ID.&lt;/p&gt; &lt;h2&gt;4. Validate Ansible Lightspeed&lt;/h2&gt; &lt;p&gt;On the bottom-right corner, you should see Lightspeed (Figure 4). Ansible Lightspeed is ready for you to write playbooks.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture4.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture4.png?itok=PdGv-GJb" width="600" height="181" alt="lightspeed" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Lightspeed reflected on Visual Studio Code.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;5. How to trigger Ansible Lightspeed&lt;/h2&gt; &lt;p&gt;Lightspeed will recommend context based on the text you type in the name field in the playbook section. Once you hit the&lt;strong&gt; Enter &lt;/strong&gt;key, you will get the suggestion as faded text. Hit the &lt;strong&gt;Tab &lt;/strong&gt;key if everything looks good to you.&lt;/p&gt; &lt;p&gt;The first example is "Installing Minikube on an RHEL server." To generate a playbook (Figure 5), make sure you have the format of the playbook as shown below, like defining hosts, permissions of become value, and so on:&lt;/p&gt; &lt;div&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;- name: Lightspeed_demo   hosts: rhel   become: true   tasks:     - name: Install apache httpd server and enable it       ansible.builtin.package:         name: httpd         state: present     - name: Install minikube on rhel server       ansible.builtin.package:         name:           https://storage.googleapis.com/minikube/releases/latest/minikube-latest.x86_64.rpm         state: present&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture5.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture5.png?itok=xmindnjQ" width="600" height="405" alt="playbook-1" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Trigger Ansible Lightspeed to generate the playbook.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The second example is installing the &lt;code&gt;kubectl&lt;/code&gt; and &lt;code&gt;oc&lt;/code&gt; command-line interface (CLI) tools on a server. As in the previous example, we are defining topics about requirements in the name field, Ansible Lightspeed is generating and suggesting to us the context of Ansible playbook. If you are fine with the suggestion, press the &lt;strong&gt;Tab&lt;/strong&gt; key to accept.&lt;/p&gt; &lt;div&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;- name: Lightspeed_demo   hosts: rhel   become: true   tasks:     - name: Include redhar.rhel_system_roles.cockpit       ansible.builtin.include_role:             name: redhar.rhel_system_roles.cockpit     - name: Install apache httpd server and enable it       ansible.builtin.package:         name: httpd         state: present     - name: Install minikube on rhel server       ansible.builtin.package:         name:           https://storage.googleapis.com/minikube/releases/latest/minikube-latest.x86_64.rpm         state: present     - name: Install oc &amp; kubectl       ansible.builtin.package:         name: "{{ item }}"         state: present       loop:         - https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/openshift-client-linux.rpm         - kubectl         - atomic-openshift-utils     - name: Install and start the openshift CRC cluster on my rhel machine       ansible.builtin.command:         cmd: /usr/local/bin/oc adm ctnplc -n openshift-cluster-api -l k8s-app=openshift-cluster-api         chdir: /home/ano-user/openshift-cluster-api&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture6.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture6.png?itok=Ij0gnTrJ" width="600" height="424" alt="playbook-2" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: Red Hat OpenShift Local cluster install using Ansible Lightspeed.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Lightspeed also provides source code recommendations in the debugging window beside the terminal. It shows more information if you extend it, as shown in Figure 7.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture7.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture7.png?itok=MkA3jl7N" width="600" height="416" alt="lightspeed source" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: Ansible Lightspeed code recommendation sources.  &lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Continue your automation journey with Ansible Automation Platform&lt;/h2&gt; &lt;p&gt;Get started with Ansible Automation Platform by exploring interactive &lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;hands-on labs&lt;/a&gt;. &lt;a href="https://developers.redhat.com/products/ansible/download#ansibleways"&gt;Download Ansible Automation Platform at no cost&lt;/a&gt; and begin your automation journey today.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/22/enhance-ansible-development-experience-lightspeed" title="Enhance Ansible development experience with Lightspeed"&gt;Enhance Ansible development experience with Lightspeed&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Nagesh Rathod</dc:creator><dc:date>2023-08-22T07:00:00Z</dc:date></entry><entry><title type="html">Q3 2023 RESTEasy Quarterly Releases</title><link rel="alternate" href="https://resteasy.dev/2023/08/21/resteasy-releases/" /><author><name /></author><id>https://resteasy.dev/2023/08/21/resteasy-releases/</id><updated>2023-08-21T18:11:11Z</updated><dc:creator /></entry><entry><title>How to use Ansible Automation Platform from GCP Marketplace</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/21/how-use-ansible-automation-platform-gcp-marketplace" /><author><name>Deepankar Jain, Himanshu Yadav</name></author><id>931cb792-207d-46e9-9c81-ac6eef13aa85</id><updated>2023-08-21T07:00:00Z</updated><published>2023-08-21T07:00:00Z</published><summary type="html">&lt;p&gt;In this article, you will learn how to use &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; from the Google Cloud Platform (GCP) Marketplace to automatically provision GCP resources.&lt;/p&gt; &lt;h2&gt;Overview of Ansible Automation Platform on GCP&lt;/h2&gt; &lt;p&gt;Red Hat Ansible Automation Platform is available on the Google Cloud Platform (GCP) Marketplace as a self-managed offering that enables enterprise-wide &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; with the benefits of &lt;a href="https://www.redhat.com/en/technologies/management/ansible/google-cloud"&gt;Ansible Automation Platform deployed on GCP cloud&lt;/a&gt;. This offering integrates seamlessly with native GCP APIs and the full Ansible collection for GCP, co-developed and security-tested by Google and Red Hat.&lt;/p&gt; &lt;p&gt;By combining the power of &lt;a href="https://developers.redhat.com/products/rhel/"&gt;Red Hat Enterprise Linux&lt;/a&gt;, &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, and GCP tools, developers can effectively scale their cloud infrastructure and leverage technologies like &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, and hybrid cloud architecture. The collaboration between Red Hat and GCP offers a hybrid cloud environment that simplifies IT management, reduces complexity, and streamlines innovation.&lt;/p&gt; &lt;h2&gt;4 components of Ansible Automation Platform&lt;/h2&gt; &lt;p&gt;You can obtain the Ansible Automation Platform self-managed offering from &lt;a href="https://console.cloud.google.com/marketplace/product/redhat-marketplace/rhaap2?project=developerbu"&gt;GCP&lt;/a&gt;. Once you have the subscription, refer to the comprehensive &lt;a href="https://access.redhat.com/documentation/en-us/ansible_on_clouds/2.3/html/red_hat_ansible_automation_platform_from_gcp_marketplace_guide/index"&gt;documentation guide&lt;/a&gt; to set up and configure your Ansible Automation Platform on GCP.&lt;/p&gt; &lt;h3&gt;1. Execution environment&lt;/h3&gt; &lt;p&gt;Ansible Playbooks run on the execution environment platform. It includes everything needed to run the Ansible Automation Platform, including the runtime environment and dependencies. The Ansible Automation Platform provides a default execution environment that includes many commonly used modules and plug-ins.&lt;/p&gt; &lt;p&gt;However, you can also create custom execution environments tailored to your specific needs (Figure 1). This allows you to include only the modules and plug-ins required for your use case, reducing the size of the environment and minimizing security risks. You can manage the execution environment through the Ansible Automation Platform web console or the execution environment builder.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-04-27_11-58-11.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-04-27_11-58-11.png?itok=xXki-AuG" width="600" height="295" alt="Figure 1: The Execution Environment page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The execution environment page of Ansible Automation Platform.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To learn more about the execution environment and execution environment builder, refer to the &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/execution_environments.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;2. Inventories&lt;/h3&gt; &lt;p&gt;An inventory is a collection of hosts and groups managed and orchestrated by Ansible Automation Platform. It is used to define and organize the hosts and groups, targeted during an automation job. An inventory can be a static file, a dynamic inventory script, or an inventory plug-in.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A static inventory file is a simple text file that lists the hosts and their attributes.&lt;/li&gt; &lt;li&gt;A dynamic inventory script retrieves the inventory information from a third-party system or cloud provider in real time.&lt;/li&gt; &lt;li&gt;Inventory plug-ins provided for specific platforms, such as GCP, Amazon Web Services (AWS) and Microsoft Azure.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In addition to hosts and groups, an inventory can also contain variables that are specific to a host or group. These variables can be used to customize how Ansible interacts with each host or group during a job.&lt;/p&gt; &lt;p&gt;As shown in Figure 2, inventories can be created, imported, and synchronized from external sources, such as cloud providers and configuration management databases (CMDBs). To learn more about inventories and how to create, manage, and work with them, refer to the official &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/inventories.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-05-22_10-23-30.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-05-22_10-23-30.png?itok=vznwedUp" width="600" height="281" alt="The Inventory page in Ansible Automation Platform." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The Inventory page in Ansible Automation Platform.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The Inventory Page&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To learn more about Inventories and how to create, manage and work with them, refer to the official &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/inventories.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;3. Credentials&lt;/h3&gt; &lt;p&gt;In the credentials section of Ansible Automation Platform, you can manage and store sensitive information, such as usernames, passwords, and private keys. These credentials can be used in your playbooks and roles to authenticate with remote hosts, cloud providers, Kubernetes and OpenShift clusters, and other systems.&lt;/p&gt; &lt;p&gt;You can create a variety of credential types (Figure 3), such as SSH, sudo, GCP access keys, tokens, certificates, endpoints, and more. These credentials can be associated with a specific organization, project, or even a specific playbook or role. By using the credential section, you can centrally manage and secure your sensitive data, making it easier to rotate or revoke credentials as needed.&lt;/p&gt; &lt;p&gt;To learn more about using credentials in Ansible Automation Platform, refer to the official &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/credentials.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-04-27_12-15-44.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-04-27_12-15-44.png?itok=ZnPJmu_E" width="600" height="241" alt="Figure 3: The Credentials Page" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: The Credentials Page&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;4. Projects&lt;/h3&gt; &lt;p&gt;In the projects section of Ansible Automation Platform, you can manage your automation content, including playbooks, roles, collections, modules, and plug-ins. A project is a collection of related content, such as all the playbooks, roles, and other files that are related to a specific task or application.&lt;/p&gt; &lt;p&gt;Projects can be created and managed through the Ansible Automation Platform web UI (Figure 4) or by utilizing the AWX CLI tool. They can be associated with a source control repository, such as Git or SVN, for version control and collaborative development.&lt;/p&gt; &lt;p&gt;Once you have a project set up, you can easily manage and organize your automation content, collaborate with other users, and ensure that your automation runs consistently across your infrastructure. You can also use projects to deploy automation content to remote servers, such as virtual machines or containers, for testing and production use.&lt;/p&gt; &lt;p&gt;For more information on managing projects in Ansible Automation Platform, you can refer to the &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/projects.html"&gt;user guide&lt;/a&gt;.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-04-27_12-17-57.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-04-27_12-17-57.png?itok=1b0k5v0F" width="600" height="132" alt="Figure 4: Project Source Control Page" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: Configuring the project source control.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Project Source Control Page&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Job and workflow templates&lt;/h2&gt; &lt;p&gt;Job templates and workflow templates are powerful features of the Ansible Automation Platform that enable IT teams to automate and orchestrate their infrastructure management tasks.&lt;/p&gt; &lt;p&gt;A job template is a predefined set of tasks that can be executed on one or more hosts, making it a powerful tool for automating repetitive tasks, such as software installations or configuration changes. With job templates, we can define the tasks to be executed, the hosts on which they should run, and any required input parameters, all through an intuitive user interface.&lt;/p&gt; &lt;p&gt;On the other hand, workflow templates allow you to chain together multiple job templates to create a more complex process or workflow. With workflow templates, you can automate even the most complex tasks, such as deploying a multi-tiered application with multiple components and dependencies.&lt;/p&gt; &lt;p&gt;Together, job and workflow templates provide a comprehensive automation solution that streamlines IT operations and ensures that your infrastructure is configured and maintained consistently and efficiently.&lt;/p&gt; &lt;h2&gt;More resources&lt;/h2&gt; &lt;p&gt;To learn about using&lt;strong&gt; &lt;/strong&gt;job templates to create Google Cloud Platform (GCP) instances with Ansible Automation Platform, check out our article, &lt;a href="https://developers.redhat.com/articles/2023/04/27/step-step-guide-creating-gcp-instance-using-ansible-automation-platform"&gt;How to create a GCP instance using Ansible Automation&lt;/a&gt;. For information on creating instances using workflow templates, read our article, &lt;a href="https://developers.redhat.com/articles/2023/04/27/step-step-guide-creating-gcp-instance-workflow-using-ansible-automation"&gt;How to create a GCP instance via workflow and Ansible&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you are looking to understand automation more in-depth, you can refer to the &lt;a href="https://developers.redhat.com/e-books/it-executives-guide-automation"&gt;IT executive's guide to automation&lt;/a&gt; ebook, which provides a comprehensive overview of automation and its impact on businesses. If you're new to &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Ansible Automation Platform&lt;/a&gt;, you can &lt;a href="https://developers.redhat.com/products/ansible/download"&gt;download&lt;/a&gt; it and &lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;get started by exploring interactive labs at no cost&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/21/how-use-ansible-automation-platform-gcp-marketplace" title="How to use Ansible Automation Platform from GCP Marketplace"&gt;How to use Ansible Automation Platform from GCP Marketplace&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Deepankar Jain, Himanshu Yadav</dc:creator><dc:date>2023-08-21T07:00:00Z</dc:date></entry><entry><title type="html">How to analyze large Java Heap Dumps</title><link rel="alternate" href="https://www.mastertheboss.com/java/how-to-parse-large-java-heap-dumps/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/java/how-to-parse-large-java-heap-dumps/</id><updated>2023-08-17T09:26:15Z</updated><content type="html">One critical tool in diagnosing memory-related issues is the Heap dump, a snapshot of an application’s memory at a particular point in time. However, as applications become larger and more intricate, heap dumps can also become massive and challenging to analyze. In this article, we will learn how to examine Heap Dump data even with ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How Testing Farm makes testing your upstream project easier</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/17/how-testing-farm-makes-testing-your-upstream-project-easier" /><author><name>David Kornel, Jakub Stejskal</name></author><id>85c81dff-bf72-479e-a422-b8b89c71a12a</id><updated>2023-08-17T07:00:00Z</updated><published>2023-08-17T07:00:00Z</published><summary type="html">&lt;p&gt;Although continuous integration is widely used in public projects, it has many limitations such as the size of resources or execution time. For instance, when you use GitHub actions free tier, you are limited to agents with 2 cpu and 7 GB of memory and total execution time of 2000 minutes per month. Other CI providers, like Azure Pipelines or Travis CI, have similar limitations. Continuous integration systems are popular, but free tier is not enough for some use cases. Fortunately, there is the Testing Farm for community projects maintained or co-maintained by Red Hat or Fedora/CentOS projects. &lt;/p&gt; &lt;p&gt;In this article, we will focus on how to use Testing Farm for projects with written and managed tests in different testing frameworks like JUnit. &lt;/p&gt; &lt;h2&gt;What is Testing Farm?&lt;/h2&gt; &lt;p&gt;&lt;a href="https://docs.testing-farm.io/general/0.1/index.html"&gt;Testing Farm&lt;/a&gt; is an open-source testing system offered as a service. You can use Testing Farm in various ways such as:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Run tests using the CLI from your local machine to create a testing machine.&lt;/li&gt; &lt;li&gt;Integrate it into the project as a CI check via &lt;a href="https://github.com/sclorg/testing-farm-as-github-action"&gt;GitHub Actions&lt;/a&gt; or &lt;a href="https://packit.dev/"&gt;Packit&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Send HTTP requests to the service &lt;a href="https://testing-farm.gitlab.io/api/"&gt;API&lt;/a&gt; with desired requirements.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The tests are defined via &lt;a href="https://tmt.readthedocs.io/"&gt;tmt&lt;/a&gt;, a test management tool that allows users to manage and execute tests on different environments. For more details about how Fedora manages the testing via tmt check out the &lt;a href="https://docs.fedoraproject.org/en-US/ci/tmt/"&gt;Fedora CI documentation&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Strimzi e2e tests&lt;/h2&gt; &lt;p&gt;As a reference project, we use &lt;a href="https://github.com/strimzi/strimzi-kafka-operator"&gt;strimzi-kafka-operator&lt;/a&gt; and test suite. The test suite is written in Java and uses the Junit5 test framework. As an execution environment, it requires a Kubernetes platform such as Red Hat OpenShift, Minikube, EKS, or similar. Due to these requirements, it is not easy to run the tests with the full potential of parallel tests execution on Azure (used as the main CI tool in Strimzi organization).&lt;/p&gt; &lt;h2&gt;How to onboard&lt;/h2&gt; &lt;p&gt;As the first step, you need access to the Testing Farm API. You should follow the&lt;a href="https://docs.testing-farm.io/general/0.1/onboarding.html"&gt; onboarding guide&lt;/a&gt;, part of the official documentation.&lt;/p&gt; &lt;h3&gt;Get the secret&lt;/h3&gt; &lt;p&gt;This secret is specific to the user and used for authentication and authorization to the service.&lt;/p&gt; &lt;h3&gt;Add tmt to your project&lt;/h3&gt; &lt;p&gt;Even if users don't use tmt for test case management, there are several parts to implement in the project to make Testing Farm understand your needs.&lt;/p&gt; &lt;p&gt;You have to mark the root folder in your project with the &lt;code&gt;.fmf&lt;/code&gt; directory, this can be done by using the &lt;code&gt;tmt init&lt;/code&gt; command. For more details, check our configuration in the Strimzi &lt;a href="https://github.com/strimzi/strimzi-kafka-operator/tree/main/.fmf"&gt;repository&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Create a test plan&lt;/h3&gt; &lt;p&gt;The plan for Testing Farm contains a test plan definition. This definition is composed of hardware requirements, preparation steps for creating a VM executor, and specific plans. The specific plan defines selectors for tests that should be executed. As part of preparation steps, we also spin-up the Minikube cluster and build and push Strimzi container images used during testing. On the bottom of the file, we specify specific test plan names and configuration for executions of specific tests defined later in this article.&lt;/p&gt; &lt;p&gt;The following is an example of our plan:&lt;/p&gt; &lt;pre&gt; # TMT test plan definition # https://tmt.readthedocs.io/en/latest/overview.html # Baseline common for all test plans ####################################################################### summary: Strimzi test suite discover: how: fmf # Required HW provision: hardware: memory: "&gt;= 24 GiB" cpu: processors: "&gt;= 8" # Install required packages and scripts for running strimzi suite prepare: - name: Install packages how: install package: - wget - java-17-openjdk-devel - xz - make - git - zip - coreutils ... ommited - name: Build strimzi images how: shell script: | # build images eval $(minikube docker-env) ARCH=$(uname -m) if [[ ${ARCH} == "aarch64" ]]; then export DOCKER_BUILD_ARGS="--platform linux/arm64 --build-arg TARGETPLATFORM=linux/arm64" fi export MVN_ARGS="-B -DskipTests -Dmaven.javadoc.skip=true --no-transfer-progress" export DOCKER_REGISTRY="localhost:5000" export DOCKER_ORG="strimzi" export DOCKER_TAG="test" make java_install make docker_build make docker_tag make docker_push # Discover tmt defined tests in tests/ folder execute: how: tmt # Post install step to copy logs finish: how: shell script:./systemtest/tmt/scripts/copy-logs.sh ####################################################################### /smoke: summary: Run smoke strimzi test suite discover+: test: - smoke /regression-operators: summary: Run regression strimzi test suite discover+: test: - regression-operators /regression-components: summary: Run regression strimzi test suite discover+: test: - regression-components /kraft-operators: summary: Run regression kraft strimzi test suite discover+: test: - kraft-operators /kraft-components: summary: Run regression kraft strimzi test suite discover+: test: - kraft-components /upgrade: summary: Run upgrade strimzi test suite provision: hardware: memory: "&gt;= 8 GiB" cpu: processors: "&gt;= 4" discover+: test: - upgrade &lt;/pre&gt; &lt;p&gt;To see the full version, refer to &lt;a href="https://github.com/strimzi/strimzi-kafka-operator/blob/main/systemtest/tmt/plans/main.fmf"&gt;our repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;How to execute tests&lt;/h2&gt; &lt;p&gt;To execute tests, we create a &lt;a href="https://github.com/strimzi/strimzi-kafka-operator/blob/main/systemtest/tmt/tests/strimzi/test.sh"&gt;simple shell script&lt;/a&gt; which calls the maven command to execute the Strimzi test suite. We also define metadata for tests which specify environment variables for test suites and other configurations like timeout.&lt;/p&gt; &lt;p&gt;In this test metadata example, you can see that we specify default env variables and configuration and specific tests override this configuration.&lt;/p&gt; &lt;pre&gt; test: ./test.sh duration: 2h environment: DOCKER_ORG: "strimzi" DOCKER_TAG: "test" TEST_LOG_DIR: "systemtest/target/logs" TESTS: "" TEST_GROUPS: "" EXCLUDED_TEST_GROUPS: "loadbalancer" CLUSTER_OPERATOR_INSTALLTYPE: bundle adjust: - environment+: EXCLUDED_TEST_GROUPS: "loadbalancer,arm64unsupported" when: arch == aarch64, arm64 /smoke: summary: Run smoke strimzi test suite tag: [smoke] duration: 20m tier: 1 environment+: TEST_PROFILE: smoke /upgrade: summary: Run upgrade strimzi test suite tag: [strimzi, kafka, upgrade] duration: 5h tier: 2 environment+: TEST_PROFILE: upgrade /regression-operators: summary: Run regression strimzi test suite tag: [strimzi, kafka, regression, operators] duration: 10h tier: 2 environment+: TEST_PROFILE: operators /regression-components: summary: Run regression strimzi test suite tag: [strimzi, kafka, regression, components] duration: 12h tier: 2 environment+: TEST_PROFILE: components /kraft-operators: summary: Run regression kraft strimzi test suite tag: [strimzi, kafka, kraft, operators] duration: 8h tier: 2 environment+: TEST_PROFILE: operators STRIMZI_FEATURE_GATES: "+UseKRaft,+StableConnectIdentities" /kraft-components: summary: Run regression kraft strimzi test suite tag: [strimzi, kafka, kraft, components] duration: 8h tier: 2 environment+: TEST_PROFILE: components STRIMZI_FEATURE_GATES: "+UseKRaft,+StableConnectIdentities" &lt;/pre&gt; &lt;h2&gt;Run Testing Farm from the command line&lt;/h2&gt; &lt;p&gt;Finally, we can try our changes via cmd. We need to install the Testing Farm CLI tool and API token. To install the Testing Farm CLI, follow this&lt;a href="https://gitlab.com/testing-farm/cli#user-content-installation"&gt; guide&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For example, we can run a smoke test on fedora-38 with both architectures  (x86_64 and aarch64) with one command:&lt;/p&gt; &lt;pre&gt; $ testing-farm request --compose Fedora-38 --git-url https://github.com/strimzi/strimzi-kafka-operator.git --git-ref main --arch x86_64,aarch64 --plan smoke 📦 repository https://github.com/strimzi/strimzi-kafka-operator.git ref main test-type fmf 💻 Fedora-38 on x86_64 💻 Fedora-38 on aarch64 🔎 api https://api.dev.testing-farm.io/v0.1/requests/*******-40ba-8a45-3c1fbe3da73a 💡 waiting for request to finish, use ctrl+c to skip 👶 request is waiting to be queued 👷 request is queued 🚀 request is running 🚢 artifacts https://artifacts.dev.testing-farm.io/*******-40ba-8a45-3c1fbe3da73a &lt;/pre&gt; &lt;p&gt;When the test run is completed, we can see the results in the web UI, as shown in Figure 1.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-06-15_at_13.33.12.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-06-15_at_13.33.12.png?itok=4rY_5r-C" width="600" height="340" alt="Testing farm web UI" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The testing farm job results in the UI.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;How Packit service simplifies integration&lt;/h2&gt; &lt;p&gt;Testing Farm deals with resource problems, but we still need to deal with execution time limits. In Strimzi, our full regression takes around 20 hours, which is insane to run on GitHub Actions, and it is quite complicated to run it on Azure Pipelines. Here is where &lt;a href="https://packit.dev/"&gt;Packit&lt;/a&gt; service comes in.&lt;/p&gt; &lt;p&gt;Packit is an open-source project that eases the integration of your project with Fedora Linux, CentOS Stream, and other distributions. Packit is mostly used by projects that build RPM packages. However, it can be easily used only for triggering tests on Testing Farm.&lt;/p&gt; &lt;p&gt;To use Packit service, users should follow the&lt;a href="https://packit.dev/docs/guide/"&gt; onboarding guide&lt;/a&gt;. We are using Packit on GitHub to install the application and grant access to our organization.&lt;/p&gt; &lt;p&gt;After finishing the onboarding, users can create a configuration file for Packit in the repository. Because we want to only run tests, we use &lt;code&gt;jobs: tests&lt;/code&gt;, but you can use different kinds of jobs and triggers. To see the full options for Packit, we suggest checking the&lt;a href="https://packit.dev/docs/configuration/"&gt; documentation&lt;/a&gt; or the article, &lt;a href="https://developers.redhat.com/articles/2022/08/16/how-set-packit-simplify-upstream-project-integration"&gt;How to set up Packit to simplify upstream project integration&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The following is an example of our configuration file:&lt;/p&gt; &lt;pre&gt; # Default packit instance is a prod and only this is used # stg instance is present for testing new packit features in forked repositories where stg is installed. packit_instances: ["prod", "stg"] upstream_project_url: https://github.com/strimzi/strimzi-kafka-operator issue_repository: https://github.com/strimzi/strimzi-kafka-operator jobs: - job: tests trigger: pull_request # Suffix for job name identifier: "upgrade" targets: # This target is not used at all by our tests, but it has to be one of the available - https://packit.dev/docs/configuration/#aliases - centos-stream-9-x86_64 - centos-stream-9-aarch64 # We don't need to build any packages for Fedora/RHEL/CentOS, it is not related to Strimzi tests skip_build: true manual_trigger: true labels: - upgrade tf_extra_params: test: fmf: name: upgrade ####################################################################### - job: tests trigger: pull_request # Suffix for job name identifier: "regression-operators" targets: # This target is not used at all by our tests, but it has to be one of the available - https://packit.dev/docs/configuration/#aliases - centos-stream-9-x86_64 - centos-stream-9-aarch64 # We don't need to build any packages for Fedora/RHEL/CentOS, it is not related to Strimzi tests skip_build: true manual_trigger: true labels: - regression - operators - regression-operators - ro tf_extra_params: test: fmf: name: regression-operators .. ommited &lt;/pre&gt; &lt;p&gt;The full .packit.yaml file is available on &lt;a href="https://github.com/strimzi/strimzi-kafka-operator/blob/main/.packit.yaml"&gt;Strimzi GitHub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you successfully onboard to the Packit service, you will see the triggered check in the GitHub pull request, as shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-06-15_at_14.08.51.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-06-15_at_14.08.51.png?itok=SlOqodVs" width="600" height="506" alt="Trigger packit from github web UI" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: An illustration of the triggered testing farm jobs.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Testing Farm simplifies your upstream project&lt;/h2&gt; &lt;p&gt;In this article, you've learned how to set up and use Testing Farm with Packit. These two services makes building and testing your upstream project easy. Now you can on-board and use it.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/17/how-testing-farm-makes-testing-your-upstream-project-easier" title="How Testing Farm makes testing your upstream project easier"&gt;How Testing Farm makes testing your upstream project easier&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>David Kornel, Jakub Stejskal</dc:creator><dc:date>2023-08-17T07:00:00Z</dc:date></entry><entry><title>How to implement Kubernetes operators with Java Operator SDK</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/16/how-implement-kubernetes-operators-java-operator-sdk" /><author><name>Igor Troyanovsky</name></author><id>26f5ac92-87ee-4b2c-ab76-c5967fac206d</id><updated>2023-08-16T07:00:00Z</updated><published>2023-08-16T07:00:00Z</published><summary type="html">&lt;p&gt;This article will demonstrate how to implement a basic operator using &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; Operator SDK, &lt;a href="https://developers.redhat.com/products/quarkus"&gt;Quarkus&lt;/a&gt;, and Fabric8 &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; client. Operators are Kubernetes extensions that use custom resources to manage applications and their components. Operators follow Kubernetes principles, such as the control loop.&lt;/p&gt; &lt;p&gt;The operator pattern concept lets you extend the cluster's behavior without modifying the code of Kubernetes by linking controllers to one or more custom resources. Operators are clients of the Kubernetes API that act as controllers for a custom resource. &lt;/p&gt; &lt;p&gt;The purpose is to code the knowledge of a human operator who is managing a service or set of services. Human operators who look after specific applications and services have a deep knowledge of how the system ought to behave, how to deploy it, and how to react if there are problems.&lt;/p&gt; &lt;p&gt;You can find additional information in the official documentation:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/architecture/controller/"&gt;Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/#extension-patterns"&gt;Kubernetes extension patterns&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/"&gt;Kubernetes operator pattern&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Why use Java?&lt;/h2&gt; &lt;p&gt;While &lt;a href="https://developers.redhat.com/topics/go"&gt;Golang&lt;/a&gt; remains the most widely used language for implementing operators and controllers, not everyone is familiar with its concepts or pointers and reference style similar to &lt;a href="https://developers.redhat.com/topics/c"&gt;C&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Java is very common in the software world. It uses a virtual machine to separate the programmer from the hardware and its object-oriented concepts highly human readable. Also, Fabric8, the popular Kubernetes client used in Java, has capabilities resembling Golang's clients.&lt;/p&gt; &lt;h2&gt;The Java Operator SDK&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://sdk.operatorframework.io/docs/overview/"&gt;Operator SDK&lt;/a&gt; is capable of automatically generating a lot of the boilerplate code needed for operator implementation. This allows the user to focus on modeling and coding the knowledge, without worrying about network interaction with Kubernetes. Plug-ins are supported to extend the SDK's options. We will focus specifically on the Quarkus plug-in.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://quarkus.io/"&gt;Quarkus Java framework&lt;/a&gt; offers fast application start-up times, low memory consumption, and lower space requirements for native images. This should get our operator up and handling our custom resources efficiently. Quarkus also uses &lt;a href="https://www.graalvm.org/"&gt;GraalVM&lt;/a&gt;, which supports changing code while the application is running.&lt;/p&gt; &lt;h2&gt;Let's get started building our operator&lt;/h2&gt; &lt;p&gt;We will create a simple Java operator, using the tools previously mentioned. This hands-on section demonstrates how the tooling helps us get a working extension to K8s API in a relatively quick way.&lt;/p&gt; &lt;h3&gt;Prerequisites&lt;/h3&gt; &lt;ul&gt;&lt;li&gt;Operator SDK and Maven should be installed on your system. If you are using macOS, these can be installed with &lt;code&gt;brew&lt;/code&gt;. For &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt;, use the package managers available for your distribution such as &lt;code&gt;dnf&lt;/code&gt; or &lt;code&gt;apt&lt;/code&gt;, or download them from their websites: &lt;a href="https://sdk.operatorframework.io/docs/installation/#install-from-github-release"&gt;Operator SDK&lt;/a&gt;, &lt;a href="https://maven.apache.org/download.cgi"&gt;Maven&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Connection to a Kubernetes or &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; cluster via kubeconfig.&lt;/li&gt; &lt;li&gt;An IDE you are comfortable with should be available. In this example we will use VS Code with common Java extensions.&lt;/li&gt; &lt;li&gt;We recommend familiarizing yourself with the &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#-strong-api-groups-strong-"&gt;Group Version Kind&lt;/a&gt; concept of Kubernetes resources.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;The use case&lt;/h3&gt; &lt;p&gt;Example.com, the company we work with, has a product named Echo, which repeats a user's input.  Deploying an Echo instance normally requires complex input and some logic.  We would like to code the knowledge needed to repeat the input in a new Kubernetes custom resource (Figure 1).&lt;/p&gt; &lt;h3&gt;Step 1: Scaffolding the first Java operator&lt;/h3&gt; &lt;p&gt;Create an empty directory named &lt;code&gt;echo-operator&lt;/code&gt;. Then &lt;code&gt;cd&lt;/code&gt; into it and run:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;operator-sdk init --plugins quarkus --domain example.com --project-name echo-operator&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, we'll follow up with scaffolding our API and open the IDE:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;operator-sdk create api --group example --version v1 --kind EchoResource&lt;/code&gt;&lt;/pre&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/239216100-db6b2537-951c-4cd7-99c0-c52537561480.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/239216100-db6b2537-951c-4cd7-99c0-c52537561480.png?itok=uIi6mNUn" width="600" height="280" alt="IDE Project" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: IDE Project&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Step 2: Custom resources in Java code&lt;/h3&gt; &lt;p&gt;Focusing on the files created under &lt;code&gt;src/main/java&lt;/code&gt;, let's look at the structure bottom-to-top. &lt;code&gt;EchoResourceSpec&lt;/code&gt; is the spec inside our custom resource. This is where the users will be providing their input.&lt;/p&gt; &lt;p&gt;Add the following field with a Getter and Setter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; private String inputMessage;     public String getInputMessage() {         return inputMessage;     }     public void setInputMessage(String inputMessage) {         this.inputMessage = inputMessage;     }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;EchoResourceStatus&lt;/code&gt; is the output status our operator returns back to the user. Add the following output field:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;     private String echoMessage;     public String getEchoMessage() {         return echoMessage;     }     public void setEchoMessage(String echoMessage) {         this.echoMessage = echoMessage;     }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;EchoResource&lt;/code&gt; is the Java class representing our custom resource. It extends the &lt;code&gt;Fabric8&lt;/code&gt; class &lt;code&gt;CustomResource&lt;/code&gt; for our spec and status. No changes required here.&lt;/p&gt; &lt;p&gt;Finally, we'll simulate an Echo Resource input given by the user.&lt;/p&gt; &lt;p&gt;Create a new file named &lt;code&gt;cr-test-echo-resource.yaml&lt;/code&gt; under &lt;code&gt;src/test/resources&lt;/code&gt; and paste the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: example.example.com/v1 kind: EchoResource metadata:   name: test-echo-resource spec:   inputMessage: "Hello from test-echo-resource" &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 3: How Operator SDK implements the reconciler&lt;/h3&gt; &lt;p&gt;Let's open the file named &lt;code&gt;EchoResourceReconciler&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;This class implements Operator SDK's &lt;code&gt;Reconciler&lt;/code&gt; method for our echo resource. It is required to implement the method &lt;code&gt;reconcile()&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;We will implement the control loop here with our knowledge about the Echo product, which repeats the user's input (Figure 2).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/239224753-9c76a290-014d-42a3-a071-92f40dfd358f.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/239224753-9c76a290-014d-42a3-a071-92f40dfd358f.png?itok=5WJZdLZM" width="600" height="357" alt="Reconciler" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: Reconciler&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Add the following code for convenience:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;private static final Logger log = LoggerFactory.getLogger(EchoResourceReconciler.class); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then the implementation of &lt;code&gt;reconcile&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;log.info("This is the control loop of the echo-operator. resource message is {}", resource.getSpec().getInputMessage()); if (reconcileStatus(resource,context)){       return UpdateControl.updateStatus(resource); } return UpdateControl.noUpdate(); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And finally the implementation of handling status:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;private boolean reconcileStatus(EchoResource resource, Context&lt;EchoResource&gt; context) {     String desiredMsg = resource.getSpec().getInputMessage();     if (resource.getStatus() == null){       // initialize if needed       resource.setStatus(new EchoResourceStatus());       resource.getStatus().setEchoMessage("");     }     if (!resource.getStatus().getEchoMessage().equalsIgnoreCase(desiredMsg)){        // the status needs to be updated with a new echo message        resource.getStatus().setEchoMessage(desiredMsg);        log.info("Setting echo resource status message to {}", desiredMsg);        // return true to signal the need to update status in Kubernetes        return true;     }     return false;   } &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 4: Testing a custom resource with live coding&lt;/h3&gt; &lt;p&gt;For convenience, we will instruct Quarkus to create the custom resource definition on our cluster, in case it doesn't exist. Open &lt;code&gt;src/main/resources/application.properties&lt;/code&gt; and change &lt;code&gt;quarkus.operator-sdk.crd.apply&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Now, let's run Quarkus via Maven as follows:&lt;/p&gt; &lt;p&gt;&lt;code&gt;mvn clean compile &amp;&amp; mvn quarkus:dev&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The controller is now running and ready to accept user input. We can follow up with our test resource: &lt;code&gt;kubectl apply -f src/test/resources/cr-test-echo-resource.yaml&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Observe the output and check the &lt;code&gt;status&lt;/code&gt; inside the &lt;code&gt;EchoResource&lt;/code&gt; on the cluster. You can also change the input spec message again and see it updated in status.&lt;/p&gt; &lt;p&gt;We can also change the code by going back to &lt;code&gt;EchoResourceReconciler&lt;/code&gt; and modifying the log message to: &lt;code&gt;This is the reconciler of the echo-operator&lt;/code&gt;. Then press &lt;code&gt;r&lt;/code&gt; in the Quarkus terminal.&lt;/p&gt; &lt;p&gt;Feel free to change and experiment. When you are done, exit out of Quarkus using &lt;code&gt;q&lt;/code&gt; and clean up the test resource with &lt;code&gt;kubectl delete -f src/test/resources/cr-test-echo-resource.yaml&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Wrap up&lt;/h2&gt; &lt;p&gt;We have demonstrated how to implement a basic operator using the Java Operator SDK, Quarkus, and the Fabric8 Kubernetes client. You can re-run, modify code, experiment, and look at the files generated. Fabric8 is also capable of creating other resources in Kubernetes, via either a builder pattern or by reading an input template yaml.&lt;/p&gt; &lt;p&gt;Take a look at the following code snippets for additional experimentation:&lt;/p&gt; &lt;p&gt;Building a service with Fabric8's builders:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;   private boolean reconcileService(EchoResource resource, Context&lt;EchoResource&gt; context) {     String desiredName = resource.getMetadata().getName();     Service echoService = client.services().withName(desiredName).get();     if (echoService == null){       log.info("Creating a service {}", desiredName);       Map&lt;String,String&gt; labels = createLabels(desiredName);       echoService = new ServiceBuilder()        .withMetadata(createMetadata(resource, labels))        .withNewSpec()            .addNewPort()                .withName("http")                .withPort(8080)            .endPort()            .withSelector(labels)            .withType("ClusterIP")        .endSpec()        .build();     client.services().resource(echoService).createOrReplace();     return true;     }     return false;   }      private Map&lt;String, String&gt; createLabels(String labelValue) {     Map&lt;String,String&gt; labelsMap = new HashMap&lt;&gt;();     labelsMap.put("owner", labelValue);     return labelsMap;   }      private ObjectMeta createMetadata(EchoResource resource, Map&lt;String, String&gt; labels){     final var metadata=resource.getMetadata();     return new ObjectMetaBuilder()        .withName(metadata.getName())        .addNewOwnerReference()            .withUid(metadata.getUid())            .withApiVersion(resource.getApiVersion())            .withName(metadata.getName())            .withKind(resource.getKind())        .endOwnerReference()        .withLabels(labels)    .build();   } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Parsing and applying a YAML with a Kubernetes resource:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;   private void createFromYaml(String pathToYaml) throws FileNotFoundException {   // Parse a yaml into a list of Kubernetes resources   List&lt;HasMetadata&gt; result = client.load(new FileInputStream(pathToYaml)).get();   // Apply Kubernetes Resources   client.resourceList(result).createOrReplace();   } &lt;/code&gt;&lt;/pre&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/16/how-implement-kubernetes-operators-java-operator-sdk" title="How to implement Kubernetes operators with Java Operator SDK"&gt;How to implement Kubernetes operators with Java Operator SDK&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Igor Troyanovsky</dc:creator><dc:date>2023-08-16T07:00:00Z</dc:date></entry></feed>
