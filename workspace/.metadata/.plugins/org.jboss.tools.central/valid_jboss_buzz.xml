<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">How to analyze large Java Heap Dumps</title><link rel="alternate" href="https://www.mastertheboss.com/java/how-to-parse-large-java-heap-dumps/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/java/how-to-parse-large-java-heap-dumps/</id><updated>2023-08-17T09:26:15Z</updated><content type="html">One critical tool in diagnosing memory-related issues is the Heap dump, a snapshot of an applicationâ€™s memory at a particular point in time. However, as applications become larger and more intricate, heap dumps can also become massive and challenging to analyze. In this article, we will learn how to examine Heap Dump data even with ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How Testing Farm makes testing your upstream project easier</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/17/how-testing-farm-makes-testing-your-upstream-project-easier" /><author><name>David Kornel, Jakub Stejskal</name></author><id>85c81dff-bf72-479e-a422-b8b89c71a12a</id><updated>2023-08-17T07:00:00Z</updated><published>2023-08-17T07:00:00Z</published><summary type="html">&lt;p&gt;Although continuous integration is widely used in public projects, it has many limitations such as the size of resources or execution time. For instance, when you use GitHub actions free tier, you are limited to agents with 2 cpu and 7 GB of memory and total execution time of 2000 minutes per month. Other CI providers, like Azure Pipelines or Travis CI, have similar limitations. Continuous integration systems are popular, but free tier is not enough for some use cases. Fortunately,Â there is the Testing Farm for community projects maintained or co-maintained by Red Hat or Fedora/CentOS projects.Â &lt;/p&gt; &lt;p&gt;In this article, we will focus on how to use Testing Farm for projects with written and managedÂ tests in different testing frameworks like JUnit.Â &lt;/p&gt; &lt;h2&gt;What is Testing Farm?&lt;/h2&gt; &lt;p&gt;&lt;a href="https://docs.testing-farm.io/general/0.1/index.html"&gt;Testing Farm&lt;/a&gt; is an open-source testing system offered as a service. You can use Testing Farm in various ways such as:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Run tests using the CLI from your local machine to create a testing machine.&lt;/li&gt; &lt;li&gt;Integrate it into the project as a CI check via &lt;a href="https://github.com/sclorg/testing-farm-as-github-action"&gt;GitHub Actions&lt;/a&gt; or &lt;a href="https://packit.dev/"&gt;Packit&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Send HTTP requests to the service &lt;a href="https://testing-farm.gitlab.io/api/"&gt;API&lt;/a&gt; with desired requirements.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The tests are defined via &lt;a href="https://tmt.readthedocs.io/"&gt;tmt&lt;/a&gt;, a test management tool that allows users to manage and execute tests on different environments. For more details about how Fedora manages the testing via tmt check out the &lt;a href="https://docs.fedoraproject.org/en-US/ci/tmt/"&gt;Fedora CI documentation&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Strimzi e2e tests&lt;/h2&gt; &lt;p&gt;As a reference project, we use &lt;a href="https://github.com/strimzi/strimzi-kafka-operator"&gt;strimzi-kafka-operator&lt;/a&gt; and test suite. The test suite is written in Java and uses the Junit5 test framework. As an execution environment, it requires a Kubernetes platform such as Red Hat OpenShift, Minikube, EKS, or similar. Due to these requirements, it is not easy to run the tests with the full potential of parallel tests execution on Azure (used as the main CI tool in Strimzi organization).&lt;/p&gt; &lt;h2&gt;How to onboard&lt;/h2&gt; &lt;p&gt;As the first step, you need access to the Testing Farm API. You should follow the&lt;a href="https://docs.testing-farm.io/general/0.1/onboarding.html"&gt; onboarding guide&lt;/a&gt;, part of the official documentation.&lt;/p&gt; &lt;h3&gt;Get the secret&lt;/h3&gt; &lt;p&gt;This secret is specific to the user and used for authentication and authorization to the service.&lt;/p&gt; &lt;h3&gt;Add tmt to your project&lt;/h3&gt; &lt;p&gt;Even if users don't use tmt for test case management, there are several parts to implement in the project to make Testing Farm understand your needs.&lt;/p&gt; &lt;p&gt;You have to mark the root folder in your project with the &lt;code&gt;.fmf&lt;/code&gt; directory, this can be done by using the &lt;code&gt;tmt init&lt;/code&gt;Â command. For more details, check our configuration in the Strimzi &lt;a href="https://github.com/strimzi/strimzi-kafka-operator/tree/main/.fmf"&gt;repository&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Create a test plan&lt;/h3&gt; &lt;p&gt;The plan for Testing Farm contains a test plan definition. This definition is composed of hardware requirements, preparation steps for creating a VM executor, and specific plans. The specific plan defines selectors for tests that should be executed. As part of preparation steps, we also spin-up the Minikube cluster and build and push Strimzi container images used during testing. On the bottom of the file, we specify specific test plan names and configuration for executions of specific tests defined later in this article.&lt;/p&gt; &lt;p&gt;The following is an example of our plan:&lt;/p&gt; &lt;pre&gt; # TMT test plan definition # https://tmt.readthedocs.io/en/latest/overview.html # Baseline common for all test plans ####################################################################### summary: Strimzi test suite discover: how: fmf # Required HW provision: hardware: memory: "&gt;= 24 GiB" cpu: processors: "&gt;= 8" # Install required packages and scripts for running strimzi suite prepare: - name: Install packages how: install package: - wget - java-17-openjdk-devel - xz - make - git - zip - coreutils ... ommited - name: Build strimzi images how: shell script: | # build images eval $(minikube docker-env) ARCH=$(uname -m) if [[ ${ARCH} == "aarch64" ]]; then export DOCKER_BUILD_ARGS="--platform linux/arm64 --build-arg TARGETPLATFORM=linux/arm64" fi export MVN_ARGS="-B -DskipTests -Dmaven.javadoc.skip=true --no-transfer-progress" export DOCKER_REGISTRY="localhost:5000" export DOCKER_ORG="strimzi" export DOCKER_TAG="test" make java_install make docker_build make docker_tag make docker_push # Discover tmt defined tests in tests/ folder execute: how: tmt # Post install step to copy logs finish: how: shell script:./systemtest/tmt/scripts/copy-logs.sh ####################################################################### /smoke: summary: Run smoke strimzi test suite discover+: test: - smoke /regression-operators: summary: Run regression strimzi test suite discover+: test: - regression-operators /regression-components: summary: Run regression strimzi test suite discover+: test: - regression-components /kraft-operators: summary: Run regression kraft strimzi test suite discover+: test: - kraft-operators /kraft-components: summary: Run regression kraft strimzi test suite discover+: test: - kraft-components /upgrade: summary: Run upgrade strimzi test suite provision: hardware: memory: "&gt;= 8 GiB" cpu: processors: "&gt;= 4" discover+: test: - upgrade &lt;/pre&gt; &lt;p&gt;To see the full version, refer toÂ &lt;a href="https://github.com/strimzi/strimzi-kafka-operator/blob/main/systemtest/tmt/plans/main.fmf"&gt;our repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;How to execute tests&lt;/h2&gt; &lt;p&gt;To execute tests, we create a &lt;a href="https://github.com/strimzi/strimzi-kafka-operator/blob/main/systemtest/tmt/tests/strimzi/test.sh"&gt;simple shell script&lt;/a&gt; which calls the maven command to execute the Strimzi test suite. We also define metadata for tests which specify environment variables for test suites and other configurations like timeout.&lt;/p&gt; &lt;p&gt;In this test metadata example, you can see that we specify default env variables and configuration and specific tests override this configuration.&lt;/p&gt; &lt;pre&gt; test: ./test.sh duration: 2h environment: DOCKER_ORG: "strimzi" DOCKER_TAG: "test" TEST_LOG_DIR: "systemtest/target/logs" TESTS: "" TEST_GROUPS: "" EXCLUDED_TEST_GROUPS: "loadbalancer" CLUSTER_OPERATOR_INSTALLTYPE: bundle adjust: - environment+: EXCLUDED_TEST_GROUPS: "loadbalancer,arm64unsupported" when: arch == aarch64, arm64 /smoke: summary: Run smoke strimzi test suite tag: [smoke] duration: 20m tier: 1 environment+: TEST_PROFILE: smoke /upgrade: summary: Run upgrade strimzi test suite tag: [strimzi, kafka, upgrade] duration: 5h tier: 2 environment+: TEST_PROFILE: upgrade /regression-operators: summary: Run regression strimzi test suite tag: [strimzi, kafka, regression, operators] duration: 10h tier: 2 environment+: TEST_PROFILE: operators /regression-components: summary: Run regression strimzi test suite tag: [strimzi, kafka, regression, components] duration: 12h tier: 2 environment+: TEST_PROFILE: components /kraft-operators: summary: Run regression kraft strimzi test suite tag: [strimzi, kafka, kraft, operators] duration: 8h tier: 2 environment+: TEST_PROFILE: operators STRIMZI_FEATURE_GATES: "+UseKRaft,+StableConnectIdentities" /kraft-components: summary: Run regression kraft strimzi test suite tag: [strimzi, kafka, kraft, components] duration: 8h tier: 2 environment+: TEST_PROFILE: components STRIMZI_FEATURE_GATES: "+UseKRaft,+StableConnectIdentities" &lt;/pre&gt; &lt;h2&gt;Run Testing Farm from the command line&lt;/h2&gt; &lt;p&gt;Finally, we can try our changes via cmd. We need to install the Testing Farm CLI tool and API token. To install the Testing Farm CLI, follow this&lt;a href="https://gitlab.com/testing-farm/cli#user-content-installation"&gt; guide&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For example, we can run a smoke test on fedora-38 with both architecturesÂ  (x86_64 and aarch64) with one command:&lt;/p&gt; &lt;pre&gt; $ testing-farm request --compose Fedora-38 --git-url https://github.com/strimzi/strimzi-kafka-operator.git --git-ref main --arch x86_64,aarch64 --plan smoke ðŸ“¦ repository https://github.com/strimzi/strimzi-kafka-operator.git ref main test-type fmf ðŸ’» Fedora-38 on x86_64 ðŸ’» Fedora-38 on aarch64 ðŸ”Ž api https://api.dev.testing-farm.io/v0.1/requests/*******-40ba-8a45-3c1fbe3da73a ðŸ’¡ waiting for request to finish, use ctrl+c to skip ðŸ‘¶ request is waiting to be queued ðŸ‘· request is queued ðŸš€ request is running ðŸš¢ artifacts https://artifacts.dev.testing-farm.io/*******-40ba-8a45-3c1fbe3da73a &lt;/pre&gt; &lt;p&gt;When the test run is completed, we can see the results in the web UI, as shown in Figure 1.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-06-15_at_13.33.12.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-06-15_at_13.33.12.png?itok=4rY_5r-C" width="600" height="340" alt="Testing farm web UI" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The testing farm job results in the UI.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;How Packit service simplifies integration&lt;/h2&gt; &lt;p&gt;Testing Farm deals with resource problems, but we still need to deal with execution time limits. In Strimzi, our full regression takes around 20 hours, which is insane to run on GitHub Actions, and it is quite complicated to run it on Azure Pipelines. Here is whereÂ &lt;a href="https://packit.dev/"&gt;Packit&lt;/a&gt; service comes in.&lt;/p&gt; &lt;p&gt;Packit is an open-source project that eases the integration of your project with Fedora Linux, CentOS Stream, and other distributions. Packit is mostly used by projects that build RPM packages. However, it can be easily used only for triggering tests on Testing Farm.&lt;/p&gt; &lt;p&gt;To use Packit service, users should follow the&lt;a href="https://packit.dev/docs/guide/"&gt; onboarding guide&lt;/a&gt;. We are using Packit on GitHub to install the application and grant access to our organization.&lt;/p&gt; &lt;p&gt;After finishing the onboarding, users can create a configuration file for Packit in the repository. Because we want to only run tests, we use &lt;code&gt;jobs: tests&lt;/code&gt;, but you can use different kinds of jobs and triggers. To see the full options for Packit,Â we suggest checking the&lt;a href="https://packit.dev/docs/configuration/"&gt; documentation&lt;/a&gt; or the article,Â &lt;a href="https://developers.redhat.com/articles/2022/08/16/how-set-packit-simplify-upstream-project-integration"&gt;How to set up Packit to simplify upstream project integration&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The following is an example of our configuration file:&lt;/p&gt; &lt;pre&gt; # Default packit instance is a prod and only this is used # stg instance is present for testing new packit features in forked repositories where stg is installed. packit_instances: ["prod", "stg"] upstream_project_url: https://github.com/strimzi/strimzi-kafka-operator issue_repository: https://github.com/strimzi/strimzi-kafka-operator jobs: - job: tests trigger: pull_request # Suffix for job name identifier: "upgrade" targets: # This target is not used at all by our tests, but it has to be one of the available - https://packit.dev/docs/configuration/#aliases - centos-stream-9-x86_64 - centos-stream-9-aarch64 # We don't need to build any packages for Fedora/RHEL/CentOS, it is not related to Strimzi tests skip_build: true manual_trigger: true labels: - upgrade tf_extra_params: test: fmf: name: upgrade ####################################################################### - job: tests trigger: pull_request # Suffix for job name identifier: "regression-operators" targets: # This target is not used at all by our tests, but it has to be one of the available - https://packit.dev/docs/configuration/#aliases - centos-stream-9-x86_64 - centos-stream-9-aarch64 # We don't need to build any packages for Fedora/RHEL/CentOS, it is not related to Strimzi tests skip_build: true manual_trigger: true labels: - regression - operators - regression-operators - ro tf_extra_params: test: fmf: name: regression-operators .. ommited &lt;/pre&gt; &lt;p&gt;The full .packit.yaml file is available on &lt;a href="https://github.com/strimzi/strimzi-kafka-operator/blob/main/.packit.yaml"&gt;Strimzi GitHub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you successfully onboard to the Packit service, you will see the triggered check in the GitHub pull request, as shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-06-15_at_14.08.51.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-06-15_at_14.08.51.png?itok=SlOqodVs" width="600" height="506" alt="Trigger packit from github web UI" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: An illustration of the triggered testing farm jobs.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Testing Farm simplifies your upstream project&lt;/h2&gt; &lt;p&gt;In this article, you've learned how to set up and use Testing Farm with Packit. These two services makes building and testing your upstream project easy. Now you can on-board and use it.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/17/how-testing-farm-makes-testing-your-upstream-project-easier" title="How Testing Farm makes testing your upstream project easier"&gt;How Testing Farm makes testing your upstream project easier&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>David Kornel, Jakub Stejskal</dc:creator><dc:date>2023-08-17T07:00:00Z</dc:date></entry><entry><title>How to implement Kubernetes operators with Java Operator SDK</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/16/how-implement-kubernetes-operators-java-operator-sdk" /><author><name>Igor Troyanovsky</name></author><id>26f5ac92-87ee-4b2c-ab76-c5967fac206d</id><updated>2023-08-16T07:00:00Z</updated><published>2023-08-16T07:00:00Z</published><summary type="html">&lt;p&gt;This article will demonstrate how to implement a basic operator using &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; Operator SDK, &lt;a href="https://developers.redhat.com/products/quarkus"&gt;Quarkus&lt;/a&gt;, and Fabric8 &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; client. Operators are Kubernetes extensions that use custom resources to manage applications and their components. Operators follow Kubernetes principles, such as the control loop.&lt;/p&gt; &lt;p&gt;The operator pattern concept lets you extend the cluster's behavior without modifying the code of Kubernetes by linking controllers to one or more custom resources. Operators are clients of the Kubernetes API that act as controllers for a custom resource.Â &lt;/p&gt; &lt;p&gt;The purpose is to code the knowledge of a human operator who is managing a service or set of services. Human operators who look after specific applications and services have a deep knowledge of how the system ought to behave, how to deploy it, and how to react if there are problems.&lt;/p&gt; &lt;p&gt;You can find additional information in the official documentation:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/architecture/controller/"&gt;Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/#extension-patterns"&gt;Kubernetes extension patterns&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/"&gt;Kubernetes operator pattern&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Why use Java?&lt;/h2&gt; &lt;p&gt;While &lt;a href="https://developers.redhat.com/topics/go"&gt;Golang&lt;/a&gt; remains the most widely used language for implementing operators and controllers, not everyone is familiar with its concepts or pointers and reference style similar to &lt;a href="https://developers.redhat.com/topics/c"&gt;C&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Java is very common in the software world. It uses a virtual machine to separate the programmer from the hardware and its object-oriented concepts highly human readable. Also, Fabric8, the popular Kubernetes client used in Java, has capabilities resembling Golang's clients.&lt;/p&gt; &lt;h2&gt;The Java Operator SDK&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://sdk.operatorframework.io/docs/overview/"&gt;Operator SDK&lt;/a&gt; is capable of automatically generating a lot of the boilerplate code needed for operator implementation. This allows the user to focus on modeling and coding the knowledge, without worrying about network interaction with Kubernetes. Plug-ins are supported to extend the SDK's options. We will focus specifically on the Quarkus plug-in.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://quarkus.io/"&gt;Quarkus Java framework&lt;/a&gt; offers fast application start-up times, low memory consumption, and lower space requirements for native images. This should get our operator up and handling our custom resources efficiently. Quarkus also uses &lt;a href="https://www.graalvm.org/"&gt;GraalVM&lt;/a&gt;, which supports changing code while the application is running.&lt;/p&gt; &lt;h2&gt;Let's get started building our operator&lt;/h2&gt; &lt;p&gt;We will create a simple Java operator, using the tools previously mentioned. This hands-on section demonstrates how the tooling helps us get a working extension to K8s API in a relatively quick way.&lt;/p&gt; &lt;h3&gt;Prerequisites&lt;/h3&gt; &lt;ul&gt;&lt;li&gt;Operator SDK and Maven should be installed on your system. If you are using macOS, these can be installed with &lt;code&gt;brew&lt;/code&gt;. For &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt;, use the package managers available for your distribution such as &lt;code&gt;dnf&lt;/code&gt; or &lt;code&gt;apt&lt;/code&gt;, or download them from their websites: &lt;a href="https://sdk.operatorframework.io/docs/installation/#install-from-github-release"&gt;Operator SDK&lt;/a&gt;, &lt;a href="https://maven.apache.org/download.cgi"&gt;Maven&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Connection to a Kubernetes or &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; cluster via kubeconfig.&lt;/li&gt; &lt;li&gt;An IDE you are comfortable with should be available. In this example we will use VS Code with common Java extensions.&lt;/li&gt; &lt;li&gt;We recommend familiarizing yourself with the &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#-strong-api-groups-strong-"&gt;Group Version Kind&lt;/a&gt; concept of Kubernetes resources.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;The use case&lt;/h3&gt; &lt;p&gt;Example.com, the company we work with, has a product named Echo, which repeats a user's input.Â  Deploying an Echo instance normally requires complex input and some logic.Â  We would like to code the knowledge needed to repeat the input in a new Kubernetes custom resource (Figure 1).&lt;/p&gt; &lt;h3&gt;Step 1: Scaffolding the first Java operator&lt;/h3&gt; &lt;p&gt;Create an empty directory named &lt;code&gt;echo-operator&lt;/code&gt;. Then &lt;code&gt;cd&lt;/code&gt; into it and run:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;operator-sdk init --plugins quarkus --domain example.com --project-name echo-operator&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, we'll follow up with scaffolding our API and open the IDE:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;operator-sdk create api --group example --version v1 --kind EchoResource&lt;/code&gt;&lt;/pre&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/239216100-db6b2537-951c-4cd7-99c0-c52537561480.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/239216100-db6b2537-951c-4cd7-99c0-c52537561480.png?itok=uIi6mNUn" width="600" height="280" alt="IDE Project" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: IDE Project&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Step 2: Custom resources in Java code&lt;/h3&gt; &lt;p&gt;Focusing on the files created under &lt;code&gt;src/main/java&lt;/code&gt;, let's look at the structure bottom-to-top.Â &lt;code&gt;EchoResourceSpec&lt;/code&gt; is the spec inside our custom resource. This is where the users will be providing their input.&lt;/p&gt; &lt;p&gt;Add the following field with a Getter and Setter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;Â private String inputMessage; Â  Â  public String getInputMessage() { Â  Â  Â  Â  return inputMessage; Â  Â  } Â  Â  public void setInputMessage(String inputMessage) { Â  Â  Â  Â  this.inputMessage = inputMessage; Â  Â  }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;EchoResourceStatus&lt;/code&gt; is the output status our operator returns back to the user. Add the following output field:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; Â  Â  private String echoMessage; Â  Â  public String getEchoMessage() { Â  Â  Â  Â  return echoMessage; Â  Â  } Â  Â  public void setEchoMessage(String echoMessage) { Â  Â  Â  Â  this.echoMessage = echoMessage; Â  Â  }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;EchoResource&lt;/code&gt; is the Java class representing our custom resource. It extends the &lt;code&gt;Fabric8&lt;/code&gt; class &lt;code&gt;CustomResource&lt;/code&gt; for our spec and status. No changes required here.&lt;/p&gt; &lt;p&gt;Finally, we'll simulate an Echo Resource input given by the user.&lt;/p&gt; &lt;p&gt;Create a new file named &lt;code&gt;cr-test-echo-resource.yaml&lt;/code&gt;Â under &lt;code&gt;src/test/resources&lt;/code&gt;Â and paste the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: example.example.com/v1 kind: EchoResource metadata: Â  name: test-echo-resource spec: Â  inputMessage: "Hello from test-echo-resource" &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 3: How Operator SDK implements the reconciler&lt;/h3&gt; &lt;p&gt;Let's open the file named &lt;code&gt;EchoResourceReconciler&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;This class implements Operator SDK's &lt;code&gt;Reconciler&lt;/code&gt;Â method for our echo resource. It is required to implement the method &lt;code&gt;reconcile()&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;We will implement the control loop here with our knowledge about the Echo product, which repeats the user's input (Figure 2).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/239224753-9c76a290-014d-42a3-a071-92f40dfd358f.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/239224753-9c76a290-014d-42a3-a071-92f40dfd358f.png?itok=5WJZdLZM" width="600" height="357" alt="Reconciler" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: Reconciler&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Add the following code for convenience:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;private static final Logger log = LoggerFactory.getLogger(EchoResourceReconciler.class); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then the implementation of &lt;code&gt;reconcile&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;log.info("This is the control loop of the echo-operator. resource message is {}", resource.getSpec().getInputMessage()); if (reconcileStatus(resource,context)){ Â  Â  Â  return UpdateControl.updateStatus(resource); } return UpdateControl.noUpdate(); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And finally the implementation of handling status:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;private boolean reconcileStatus(EchoResource resource, Context&lt;EchoResource&gt; context) { Â  Â  String desiredMsg = resource.getSpec().getInputMessage(); Â  Â  if (resource.getStatus() == null){ Â  Â  Â  // initialize if needed Â  Â  Â  resource.setStatus(new EchoResourceStatus()); Â  Â  Â  resource.getStatus().setEchoMessage(""); Â  Â  } Â  Â  if (!resource.getStatus().getEchoMessage().equalsIgnoreCase(desiredMsg)){ Â  Â  Â  Â // the status needs to be updated with a new echo message Â  Â  Â  Â resource.getStatus().setEchoMessage(desiredMsg); Â  Â  Â  Â log.info("Setting echo resource status message to {}", desiredMsg); Â  Â  Â  Â // return true to signal the need to update status in Kubernetes Â  Â  Â  Â return true; Â  Â  } Â  Â  return false; Â  } &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 4: Testing a custom resource with live coding&lt;/h3&gt; &lt;p&gt;For convenience, we will instruct Quarkus to create the custom resource definition on our cluster, in case it doesn't exist. Open &lt;code&gt;src/main/resources/application.properties&lt;/code&gt;Â and change &lt;code&gt;quarkus.operator-sdk.crd.apply&lt;/code&gt;Â to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Now, let's run Quarkus via Maven as follows:&lt;/p&gt; &lt;p&gt;&lt;code&gt;mvn clean compile &amp;&amp; mvn quarkus:dev&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The controller is now running and ready to accept user input. We can follow up with our test resource: &lt;code&gt;kubectl apply -f src/test/resources/cr-test-echo-resource.yaml&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Observe the output and check the &lt;code&gt;status&lt;/code&gt;Â inside the &lt;code&gt;EchoResource&lt;/code&gt;Â on the cluster. You can also change the input spec message again and see it updated in status.&lt;/p&gt; &lt;p&gt;We can also change the code by going back to &lt;code&gt;EchoResourceReconciler&lt;/code&gt; and modifying the log message to: &lt;code&gt;This is the reconciler of the echo-operator&lt;/code&gt;. Then press &lt;code&gt;r&lt;/code&gt; in the Quarkus terminal.&lt;/p&gt; &lt;p&gt;Feel free to change and experiment. When you are done, exit out of Quarkus using &lt;code&gt;q&lt;/code&gt; and clean up the test resource with &lt;code&gt;kubectl delete -f src/test/resources/cr-test-echo-resource.yaml&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Wrap up&lt;/h2&gt; &lt;p&gt;We have demonstrated how to implement a basic operator using the Java Operator SDK, Quarkus, and the Fabric8 Kubernetes client. You can re-run, modify code, experiment, and look at the files generated. Fabric8 is also capable of creating other resources in Kubernetes, via either a builder pattern or by reading an input template yaml.&lt;/p&gt; &lt;p&gt;Take a look at the following code snippets for additional experimentation:&lt;/p&gt; &lt;p&gt;Building a service with Fabric8's builders:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; Â  private boolean reconcileService(EchoResource resource, Context&lt;EchoResource&gt; context) { Â  Â  String desiredName = resource.getMetadata().getName(); Â  Â  Service echoService = client.services().withName(desiredName).get(); Â  Â  if (echoService == null){ Â  Â  Â  log.info("Creating a service {}", desiredName); Â  Â  Â  Map&lt;String,String&gt; labels = createLabels(desiredName); Â  Â  Â  echoService = new ServiceBuilder() Â  Â  Â  Â .withMetadata(createMetadata(resource, labels)) Â  Â  Â  Â .withNewSpec() Â  Â  Â  Â  Â  Â .addNewPort() Â  Â  Â  Â  Â  Â  Â  Â .withName("http") Â  Â  Â  Â  Â  Â  Â  Â .withPort(8080) Â  Â  Â  Â  Â  Â .endPort() Â  Â  Â  Â  Â  Â .withSelector(labels) Â  Â  Â  Â  Â  Â .withType("ClusterIP") Â  Â  Â  Â .endSpec() Â  Â  Â  Â .build(); Â  Â  client.services().resource(echoService).createOrReplace(); Â  Â  return true; Â  Â  } Â  Â  return false; Â  } Â Â  Â  private Map&lt;String, String&gt; createLabels(String labelValue) { Â  Â  Map&lt;String,String&gt; labelsMap = new HashMap&lt;&gt;(); Â  Â  labelsMap.put("owner", labelValue); Â  Â  return labelsMap; Â  } Â Â  Â  private ObjectMeta createMetadata(EchoResource resource, Map&lt;String, String&gt; labels){ Â  Â  final var metadata=resource.getMetadata(); Â  Â  return new ObjectMetaBuilder() Â  Â  Â  Â .withName(metadata.getName()) Â  Â  Â  Â .addNewOwnerReference() Â  Â  Â  Â  Â  Â .withUid(metadata.getUid()) Â  Â  Â  Â  Â  Â .withApiVersion(resource.getApiVersion()) Â  Â  Â  Â  Â  Â .withName(metadata.getName()) Â  Â  Â  Â  Â  Â .withKind(resource.getKind()) Â  Â  Â  Â .endOwnerReference() Â  Â  Â  Â .withLabels(labels) Â  Â .build(); Â  } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Parsing and applying a YAML with a Kubernetes resource:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; Â  private void createFromYaml(String pathToYaml) throws FileNotFoundException { Â  // Parse a yaml into a list of Kubernetes resources Â  List&lt;HasMetadata&gt; result = client.load(new FileInputStream(pathToYaml)).get(); Â  // Apply Kubernetes Resources Â  client.resourceList(result).createOrReplace(); Â  } &lt;/code&gt;&lt;/pre&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/16/how-implement-kubernetes-operators-java-operator-sdk" title="How to implement Kubernetes operators with Java Operator SDK"&gt;How to implement Kubernetes operators with Java Operator SDK&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Igor Troyanovsky</dc:creator><dc:date>2023-08-16T07:00:00Z</dc:date></entry><entry><title type="html">PrimeFaces on Quarkus made simple</title><link rel="alternate" href="https://www.mastertheboss.com/web/primefaces/primefaces-on-quarkus-made-simple/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/web/primefaces/primefaces-on-quarkus-made-simple/</id><updated>2023-08-15T07:57:42Z</updated><content type="html">PrimeFaces is a renowned open-source UI component library for JSF-based web applications. It provides an extensive set of rich, customizable UI components that simplify the process of building feature-rich and visually appealing user interfaces. In this article we will learn how to integrate its library in Quarkus applications. This is the second round of article ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Set up a continuous integration pipeline with Ansible Automation Platform &amp; GitLab</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/15/continuous-integration-pipeline-ansible-gitlab" /><author><name>Nagesh Rathod</name></author><id>6a764479-4640-4244-9a38-eafab5c34d36</id><updated>2023-08-15T07:00:00Z</updated><published>2023-08-15T07:00:00Z</published><summary type="html">&lt;p&gt;In modern software development practices, &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; plays a crucial role in streamlining processes and ensuring efficient and reliable deployments. &lt;a href="https://developers.redhat.com/products/ansible/"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;Â is an enterprise automation tool that allows you to define and manage configuration as code. GitLab, on the other hand, provides a robustÂ &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;CI/CD&lt;/a&gt; platform for managing code repositories and executing continuous integration and deployment pipelines.&lt;/p&gt; &lt;p&gt;Integrating Ansible Automation Platform into GitLab CI pipelines (as illustrated in Figure 1) enables organizations to automate infrastructure provisioning and configuration management alongside their application deployments. This tutorial will guide you through the process of integrating Ansible Automation Platform in a GitLab CI pipeline.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/1_5.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/1_5.png?itok=8tKjaehd" width="600" height="259" alt="Architectural diagram" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Architecture diagram of GitLab CI/CD&lt;strong&gt;Â &lt;/strong&gt;with Ansible Automation Platform&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The pipeline starts when you commit changes in the GitLab code repository. The &lt;code&gt;gitlab-ci.yaml&lt;/code&gt; file contains the blueprints for the pipelines. The tasks for building and deploying are executed by GitLab Runner based on that file. Deployment occurs through the POST API call of Ansible's automation controller. Once the API call is made, the template will trigger and deploy the application in the appropriate environment.Â &lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;Before proceeding, make sure the following prerequisites are installed on your system:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2023/01/01/how-install-red-hat-ansible-automation-platform-rhel-9"&gt;Ansible Automation Platform&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://about.gitlab.com/install/"&gt;GitLab Runner&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;1. Create the GitLab CI Pipeline&lt;/h2&gt; &lt;p&gt;As we already know, GitLab CI is a popular tool for CI/CD. Integrating Ansible Automation Platform with GitLab CI gives you more possibilities to explore.&lt;/p&gt; &lt;p&gt;Before moving forward, make sure the GitLab Runner is live and running, as shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/2_8.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/2_8.png?itok=vaCPZIio" width="600" height="329" alt="runner up and running" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The GitLab Runner is up and running.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The following &lt;code&gt;gitlab-ci.yaml&lt;/code&gt; file includes the build and deployment stage:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;stages:Â  Â  Â  Â  Â  # List of stages for jobs, and their order of execution Â  - build Â  - deploy build-job: Â  Â  Â  # This job runs in the build stage, which runs first. Â  stage: build Â  script: Â  Â  - echo "Compiling the code..." Â  Â  - echo "Compile complete." deploy-job:Â  Â  Â  # This job runs in the deploy stage. Â  stage: deploy Â  environment: production Â  script: Â  Â  - curl -k -X POST --user '$ANSIBLE_CONTROLLER_USER:$ANSIBLE_CONTROLLER_PASSWORD' -H "Content-Type:application/json"Â  --data '{"limit":"ansible"}'Â  http://$ANSIBLE_CONTROLLER_URL/api/v2/job_templates/7/launch/ -k -L Â  Â  - echo "deployment is done"&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;2. Add Red Hat API integration&lt;/h2&gt; &lt;p&gt;Red Hat offers &lt;a href="https://developers.redhat.com/api-catalog"&gt;API catalogs and documentation&lt;/a&gt; for its products. Here, we will use the &lt;a href="https://docs.ansible.com/automation-controller/latest/html/controllerapi/authentication.html#api-o"&gt;POST API&lt;/a&gt; to call Ansible controller.Â Â &lt;/p&gt; &lt;h3&gt;Add variable in GitLab&lt;/h3&gt; &lt;p&gt;The variable is a secured way to pass your credentials in the pipeline. To add variables in the pipeline, follow these steps:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Log in to the GitLab web console and from the left menu, select &lt;strong&gt;Settings&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on theÂ &lt;strong&gt;CI/CD&lt;/strong&gt; section.&lt;/li&gt; &lt;li aria-level="1"&gt;At the center of the screen, you will see the fifth option listed as &lt;strong&gt;Variable.&lt;/strong&gt;Â Click theÂ &lt;strong&gt;Expand&lt;/strong&gt; option in front of the variable.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on &lt;strong&gt;Add Variable&lt;/strong&gt; option, then add the &lt;strong&gt;key&lt;/strong&gt; as given below andÂ add &lt;code&gt;value&lt;/code&gt; as your credentials.&lt;/li&gt; &lt;li aria-level="1"&gt;Add the following variables:&lt;/li&gt; &lt;/ol&gt;&lt;ul&gt;&lt;li class="Indent1"&gt;&lt;code&gt;$ANSIBLE_CONTROLLER_USER&lt;/code&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;&lt;code&gt;$ANSIBLE_CONTROLLER_PASSWORD&lt;/code&gt;&lt;strong&gt;Â &lt;/strong&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;&lt;code&gt;$ANSIBLE_CONTROLLER_URL&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/3_10.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/3_10.png?itok=H5XISwRV" width="600" height="189" alt="gitlab ci variables" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Adding variables for the pipeline.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;3. Start the pipeline&lt;/h2&gt; &lt;p&gt;We can see in Figure 3 that the YAML file contains the deployment stage. We'll use the &lt;a href="https://docs.ansible.com/automation-controller/latest/html/controllerapi/authentication.html#api-oauth2-auth"&gt;POST API&lt;/a&gt;Â to trigger the Ansible template. The template contains inventories, playbooks, host server credentials, secrets, etc.&lt;/p&gt; &lt;p&gt;To configure the template, refer to the following tutorials on bare-metal instances, &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; clusters, and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; clusters:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2023/03/13/how-use-automation-controller-install-ms-sql#step_4__configuring_a_projectÂ "&gt;Creating a project&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/articles/2023/02/28/how-employ-continuous-deployment-ansible-openshift#_configuring_job_templatesÂ "&gt;Configuring job templates&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;After adding the YAML, it is time to test the pipeline to ensure it is working. To test the pipeline, first commit the changes.&lt;/p&gt; &lt;p&gt;Log in to the GitLab web console and select &lt;strong&gt;CI/CD â†’ Pipeline&lt;/strong&gt; from the left menu.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/4_7.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/4_7.png?itok=WSSK3qx-" width="600" height="305" alt="done pipeline" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: A successful pipeline run.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To check that the pipeline template job executed successfully, log in to Ansible controller, navigate to &lt;strong&gt;Jobs&lt;/strong&gt;, and check for the latest job. You will get the following output in case of successful execution:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;PLAY [Application deployed] ****************************************************** TASK [Gathering Facts] ********************************************************* ok: [localhost]&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Continue your automation journey&lt;/h2&gt; &lt;p&gt;You can &lt;a href="https://developers.redhat.com/products/ansible/download"&gt;download the latest version of Ansible Automation Platform&lt;/a&gt; from our website at no cost. Get started with Ansible Automation Platform right away withÂ &lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;our interactive labs&lt;/a&gt;.Â &lt;/p&gt; &lt;p&gt;You can find more Red Hat APIs and documentation in the &lt;a href="https://developers.redhat.com/api-catalogÂ "&gt;API Catalog&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/15/continuous-integration-pipeline-ansible-gitlab" title="Set up a continuous integration pipeline with Ansible Automation Platform &amp; GitLab"&gt;Set up a continuous integration pipeline with Ansible Automation Platform &amp; GitLab&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Nagesh Rathod</dc:creator><dc:date>2023-08-15T07:00:00Z</dc:date></entry><entry><title>How to use Ansible Automation Platform from Azure Marketplace</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/14/how-use-ansible-automation-platform-azure-marketplace" /><author><name>Deepankar Jain, Himanshu Yadav</name></author><id>ff56efb1-8bb0-4b4b-b69b-09bb91e1649a</id><updated>2023-08-14T07:00:00Z</updated><published>2023-08-14T07:00:00Z</published><summary type="html">&lt;p&gt;In this article, you will learn about using the Red Hat Ansible Automation Platform from the Microsoft Azure Marketplace to automatically provision Azure resources.&lt;/p&gt; &lt;h2&gt;Overview of Ansible Automation Platform on Azure&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;Â isÂ available on the Azure Marketplace. It is a self-managed offering that enables enterprise-wide automation with the benefits of &lt;a href="https://www.redhat.com/en/technologies/management/ansible/azure"&gt;Ansible Automation Platform deployed on Azure cloud&lt;/a&gt;. This offering integrates seamlessly with native Azure services and the full ansible collection for Azure, co-developed and security-tested by Microsoft and Red Hat.&lt;/p&gt; &lt;p&gt;By combining the power of Red Hat Enterprise Linux, Red Hat OpenShift, and Azure services, developers can effectively scale their cloud infrastructure and leverage technologies like containers, Kubernetes, and hybrid cloud architecture. The collaboration between Red Hat and Azure offers a hybrid cloud environment that simplifies IT management, reduces complexity, and streamlines innovation.&lt;/p&gt; &lt;p&gt;You can obtain the Ansible Automation Platform self-managed offering fromÂ &lt;a href="https://azuremarketplace.microsoft.com/en-in/marketplace/apps/redhat.ansible-automation-platform?tab=Overview"&gt;Azure Marketplace&lt;/a&gt;. Once you have the subscription, refer to theÂ comprehensiveÂ &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/2.1/html-single/red_hat_ansible_automation_platform_on_microsoft_azure_guide/index#aap-azure-install"&gt;documentation guide&lt;/a&gt;Â to set up and configure your Ansible Automation Platform on Azure.&lt;/p&gt; &lt;h2&gt;4 Components of Ansible Automation Platform&lt;/h2&gt; &lt;p&gt;The following is an overview of four componentsÂ of Ansible Automation Platform.&lt;/p&gt; &lt;h3&gt;1. Execution environment&lt;/h3&gt; &lt;p&gt;Ansible Playbooks run on the execution environment platform. It includes everything needed to run the Ansible Automation Platform, including the runtime environment and dependencies. The Ansible Automation Platform provides a default execution environment that includes many commonly used modules and plugins.&lt;/p&gt; &lt;p&gt;However, you can also create custom execution environments tailored to your specific needs (Figure 1). This allows you to include only the modules and plugins required for your use case, reducing the size of the environment and minimizing security risks. You can manage the execution environmentÂ through the Ansible Automation Platform web console or the execution environment builder.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-04-27_11-58-11.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-04-27_11-58-11.png?itok=xXki-AuG" width="600" height="295" alt="Figure 1: The Execution Environment page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The execution environment page of Ansible Automation Platform.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To learn more about the execution environment and execution environmentÂ builder, refer to theÂ &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/execution_environments.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;2. Inventories&lt;/h3&gt; &lt;p&gt;An inventory is a collection of hosts and groups managed and orchestrated by Ansible Automation Platform. It is used to define and organize the hosts and groups, targeted during an automation job. An inventory can be a static file, a dynamic inventory script, or an inventory plugin.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A static inventory file is a simple text file that lists the hosts and their attributes.&lt;/li&gt; &lt;li&gt;A dynamic inventory script retrieves the inventory information from a third-party system or cloud provider in real time.&lt;/li&gt; &lt;li&gt;Inventory plugins providedÂ for specific platforms, such as Amazon Web Services (AWS) and Microsoft Azure.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In addition to hosts and groups, an inventory can also contain variables that are specific to a host or group. These variables can be used to customize how Ansible interacts with each host or group during a job.&lt;/p&gt; &lt;p&gt;As shown in Figure 2, inventories can be created, imported, and synchronized from external sources, such as cloud providers and configuration management databases (CMDBs). To learn more about inventories and how to create, manage, and work with them, refer to the officialÂ &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/inventories.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-05-22_10-23-30.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-05-22_10-23-30.png?itok=vznwedUp" width="600" height="281" alt="Figure 2: Inventory Page" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The Inventory page in Ansible Automation Platform.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;3. Credentials&lt;/h3&gt; &lt;p&gt;In the credentials section of Ansible Automation Platform, you can manage and store sensitive information, such as usernames, passwords, and private keys. These credentials can be used in your playbooks and roles to authenticate with remote hosts, cloud providers, Kubernetes and OpenShift clusters, and other systems.&lt;/p&gt; &lt;p&gt;You can create a variety of credential types, such as SSH, sudo, Azure access keys, tokens, certificates, endpoints, and more. These credentials can be associated with a specific organization, project, or even a specific playbook or role. By using the credential section, you can centrally manage and secure your sensitive data, making it easier to rotate or revoke credentials as needed.&lt;/p&gt; &lt;p&gt;To learn more about using credentials in Ansible Automation Platform, refer to the official &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/credentials.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;4. Projects&lt;/h3&gt; &lt;p&gt;In the projects section of Ansible Automation Platform, you can manage your automation content, including playbooks, roles, collections, modules, and plugins. A project is a collection of related content, such as all the playbooks, roles, and other files that are related to a specific task or application.&lt;/p&gt; &lt;p&gt;Projects can be created and managed through the Ansible Automation Platform web UI or by utilizing the AWX CLI tool. They can be associated with a source control repository, such as Git or SVN, for version control and collaborative development.&lt;/p&gt; &lt;p&gt;Once you have a project set up, you can easily manage and organize your automation content, collaborate with other users, and ensure that your automation runs consistently across your infrastructure. You can also use projects to deploy automation content to remote servers, such as virtual machines or containers, for testing and production use.&lt;/p&gt; &lt;p&gt;For more information on managing projects in Ansible Automation Platform, you can refer to the &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/projects.html"&gt;user guide&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Job and workflow templates&lt;/h2&gt; &lt;p&gt;Job templates and workflow templates are powerful features of the Ansible Automation Platform that enable IT teams to automate and orchestrate their infrastructure management tasks.&lt;/p&gt; &lt;p&gt;A job template is a predefined set of tasks that can be executed on one or more hosts, making it a powerful tool for automating repetitive tasks, such as software installations or configuration changes. With job templates, we can define the tasks to be executed, the hosts on which they should run, and any required input parameters, all through an intuitive user interface.&lt;/p&gt; &lt;p&gt;On the other hand, workflow templates allow you to chain together multiple job templates to create a more complex process or workflow. With workflow templates, you can automate even the most complex tasks, such as deploying a multi-tiered application with multiple components and dependencies.&lt;/p&gt; &lt;p&gt;Together, job and workflow templates provide a comprehensive automation solution that streamlines IT operations and ensures that your infrastructure is configured and maintained consistently and efficiently.&lt;/p&gt; &lt;h2&gt;Find more resources&lt;/h2&gt; &lt;p&gt;To learn about using job templates to create virtual machines in Azure with Ansible Automation Platform, check out our article,Â &lt;a href="https://developers.redhat.com/articles/2023/04/27/step-step-guide-creating-virtual-machine-microsoft-azure-using-ansible"&gt;How to use Ansible to create a VM on Azure&lt;/a&gt;.Â For information on creating virtual machines and managing infrastructure using workflow templates, read our article, &lt;a href="https://developers.redhat.com/articles/2023/04/27/step-step-guide-creating-virtual-machine-microsoft-azure-workflow-using-ansible"&gt;How to use Ansible to create a VM on Azure via workflow&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you would like to understand automation more in-depth, refer to the &lt;a href="https://developers.redhat.com/e-books/it-executives-guide-automation"&gt;IT Executive's Guide to Automation&lt;/a&gt; e-book, which provides a comprehensive overview of automation and its impact on businesses. If you're new to &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Ansible Automation Platform&lt;/a&gt;, you can &lt;a href="https://developers.redhat.com/products/ansible/download"&gt;download&lt;/a&gt; it and &lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;get started by exploring interactive labs at no cost&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/14/how-use-ansible-automation-platform-azure-marketplace" title="How to use Ansible Automation Platform from Azure Marketplace"&gt;How to use Ansible Automation Platform from Azure Marketplace&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Deepankar Jain, Himanshu Yadav</dc:creator><dc:date>2023-08-14T07:00:00Z</dc:date></entry><entry><title>Quarkus Newsletter #35 - August</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-newsletter-35/&#xA;            " /><author><name>James Cobb (https://twitter.com/insectengine)</name></author><id>https://quarkus.io/blog/quarkus-newsletter-35/</id><updated>2023-08-14T00:00:00Z</updated><published>2023-08-14T00:00:00Z</published><summary type="html">Read Ladislav Thonâ€™s article "On the Road to CDI Compatibility" to learn about the long road to make Quarkus compatible with CDI Lite. An Elsevier software engineer (Neil Stevens) writes about how to use Quarkus to improve Java functions with AWS Lambda in "Elsevier Tech: Writing a native Java Lambda...</summary><dc:creator>James Cobb (https://twitter.com/insectengine)</dc:creator><dc:date>2023-08-14T00:00:00Z</dc:date></entry><entry><title>Quarkus 3.2.4.Final released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-3-2-4-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-3-2-4-final-released/</id><updated>2023-08-11T00:00:00Z</updated><published>2023-08-11T00:00:00Z</published><summary type="html">Today, we released Quarkus 3.2.4.Final, the fourth maintenance release of our 3.2 release train. It should be a safe upgrade for anyone already using 3.2. If you are not already using 3.2, please refer to the Quarkus 3.2 migration guide. And if you are not already using 3.0, please refer...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-08-11T00:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.42.0 released!</title><link rel="alternate" href="https://blog.kie.org/2023/08/kogito-1-42-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2023/08/kogito-1-42-0-released.html</id><updated>2023-08-10T23:19:34Z</updated><content type="html">We are glad to announce that the Kogito 1.42.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Fixed an issue that could make live reload not work as expected for workflows that use OpenAPI or gRPC. * Json schema validation: * Changed library from (org,json based)Â  to (jackson based) * Support nested json schema * Python support * Added support for python method call (embedded python script support was added in previous release) * Process definition addon that allows dynamically uploading SWF files to a Quarkus deployment (Experimental) For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.31.0 artifacts are available at the . A detailed changelog for 1.42.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title>The benefits of deploying Ansible Automation Platform on AWS</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/10/benefits-deploying-ansible-automation-platform-aws" /><author><name>Deepankar Jain, Himanshu Yadav</name></author><id>7cad34ec-4b5d-4414-a3d2-639a2c8e6c2f</id><updated>2023-08-10T07:00:00Z</updated><published>2023-08-10T07:00:00Z</published><summary type="html">&lt;p&gt;In this article, you will learn how to use the Red Hat Ansible Automation Platform from the Amazon Web Services (AWS) Marketplace to automatically provision AWS resources.&lt;/p&gt; &lt;h2&gt;Overview of Ansible Automation Platform on AWSÂ &lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; is available on the Amazon Web Services (AWS) Marketplace. It is a self-managed offering that enables enterprise-wide automation with the benefits of deployingÂ &lt;a href="https://www.redhat.com/en/technologies/management/ansible/aws" target="_blank"&gt;Ansible Automation Platform on AWS cloud&lt;/a&gt;. This offering integrates seamlessly with native AWS services and the full Ansible collection for AWS, co-developed and security-tested by AWS and Red Hat.&lt;/p&gt; &lt;p&gt;By combining the power of Red Hat Enterprise Linux, Red Hat OpenShift, and AWS services, developers can effectively scale their cloud infrastructure and leverage technologies such as containers, Kubernetes, and hybrid cloud architecture. The collaboration between Red Hat and AWS offers a hybrid cloud environment that simplifies IT management, reduces complexity, and streamlines innovation.&lt;/p&gt; &lt;h2&gt;4 Components of Ansible Automation Platform&lt;/h2&gt; &lt;p&gt;You can obtain the Ansible Automation Platform offering from &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-l36q6uvlouwb4?sr=0-1&amp;ref_=beagle&amp;applicationId=AWSMPContessa"&gt;AWS Marketplace&lt;/a&gt;. Once you have the subscription, refer to the comprehensive &lt;a href="https://access.redhat.com/documentation/en-us/ansible_on_clouds/2.x/html/red_hat_ansible_automation_platform_from_aws_marketplace_guide/assembly-aap-aws-install"&gt;documentation guide&lt;/a&gt; to set up and configure your Ansible Automation Platform on AWS.&lt;/p&gt; &lt;p&gt;The following is an overview of four componentsÂ of Ansible Automation Platform.&lt;/p&gt; &lt;h3&gt;1. Execution environment&lt;/h3&gt; &lt;p&gt;Ansible Playbooks run on the execution environment platform. It includes everything needed to run the Ansible Automation Platform, including the runtime environment and dependencies. The Ansible Automation Platform provides a default execution environment that includes many commonly used modules and plugins.&lt;/p&gt; &lt;p&gt;However, you can also create custom execution environments tailored to your specific needs (Figure 1). This allows you to include only the modules and plugins required for your use case, reducing the size of the environment and minimizing security risks. You can manage the execution environmentÂ through the Ansible Automation Platform web console or the execution environment builder.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-04-27_11-58-11.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-04-27_11-58-11.png?itok=xXki-AuG" width="600" height="295" alt="A screenshot of the execution environment page of Ansible Automation Platform." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The execution environment page of Ansible Automation Platform.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To learn more about the execution environment and execution environmentÂ builder, refer to the &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/execution_environments.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;2. Inventories&lt;/h3&gt; &lt;p&gt;An inventory is a collection of hosts and groups managed and orchestrated by Ansible Automation Platform. It is used to define and organize the hosts and groups, targeted during an automation job. An inventory can be a static file, a dynamic inventory script, or an inventory plugin.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A static inventory file is a simple text file that lists the hosts and their attributes.&lt;/li&gt; &lt;li&gt;A dynamic inventory script retrieves the inventory information from a third-party system or cloud provider in real time.&lt;/li&gt; &lt;li&gt;Inventory plugins providedÂ for specific platforms, such as Amazon Web Services (AWS) and Microsoft Azure.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In addition to hosts and groups, an inventory can also contain variables that are specific to a host or group. These variables can be used to customize how Ansible interacts with each host or group during a job.&lt;/p&gt; &lt;p&gt;As shown in Figure 2, inventories can be created, imported, and synchronized from external sources, such as cloud providers and configuration management databases (CMDBs). To learn more about Inventories and how to create, manage, and work with them, refer to the officialÂ &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/inventories.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-05-22_10-23-30.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-05-22_10-23-30.png?itok=vznwedUp" width="600" height="281" alt="The Inventory page in Ansible Automation Platform." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The Inventory page in Ansible Automation Platform.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;3. Credentials&lt;/h3&gt; &lt;p&gt;In the credentials section of Ansible Automation Platform, you can manage and store sensitive information, such as usernames, passwords, and private keys. These credentials can be used in your playbooks and roles to authenticate with remote hosts, cloud providers, Kubernetes and OpenShift clusters, and other systems.&lt;/p&gt; &lt;p&gt;You can create a variety of credential types, such as SSH, sudo, AWS access keys, tokens, certificates, endpoints, and more. These credentials can be associated with a specific organization, project, or even a specific playbook or role. By using the credential section, you can centrally manage and secure your sensitive data, making it easier to rotate or revoke credentials as needed.&lt;/p&gt; &lt;p&gt;To learn more about using credentials in Ansible Automation Platform, refer to the official &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/credentials.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;4. Projects&lt;/h3&gt; &lt;p&gt;In the projects section of Ansible Automation Platform, you can manage your automation content, including playbooks, roles, collections, modules, and plugins. A project is a collection of related content, such as all the playbooks, roles, and other files that are related to a specific task or application.&lt;/p&gt; &lt;p&gt;Projects can be created and managed through the Ansible Automation Platform web UI or by utilizing the AWX CLI tool. They can be associated with a source control repository, such as Git or SVN, for version control and collaborative development.&lt;/p&gt; &lt;p&gt;Once you have a project set up, you can easily manage and organize your automation content, collaborate with other users, and ensure that your automation runs consistently across your infrastructure. You can also use projects to deploy automation content to remote servers, such as virtual machines or containers, for testing and production use.&lt;/p&gt; &lt;p&gt;For more information on managing projects in Ansible Automation Platform, you can refer to the &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/projects.html"&gt;user guide&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Job and workflow templatesÂ &lt;/h2&gt; &lt;p&gt;Job templates and workflow templates are powerful features of the Ansible Automation Platform that enable IT teams to automate and orchestrate their infrastructure management tasks.&lt;/p&gt; &lt;p&gt;A job template is a predefined set of tasks that can be executed on one or more hosts, making it a powerful tool for automating repetitive tasks, such as software installations or configuration changes. With job templates, we can define the tasks to be executed, the hosts on which they should run, and any required input parameters, all through an intuitive user interface.&lt;/p&gt; &lt;p&gt;On the other hand, workflow templates allow you to chain together multiple job templates to create a more complex process or workflow. With workflow templates, you can automate even the most complex tasks, such as deploying a multi-tiered application with multiple components and dependencies.&lt;/p&gt; &lt;p&gt;Together, job and workflow templates provide a comprehensive automation solution that streamlines IT operations and ensures that your infrastructure is configured and maintained consistently and efficiently.&lt;/p&gt; &lt;h2&gt;Find more resources&lt;/h2&gt; &lt;p&gt;To learn about using&lt;strong&gt; &lt;/strong&gt;job templates to create Amazon Web Services (AWS) EC2 instances with Ansible Automation Platform, check out our step-by-step guide for &lt;a href="https://developers.redhat.com/articles/2023/04/28/step-step-guide-creating-amazon-web-services-aws-ec2-instance-using-ansible"&gt;How to create an EC2 instance in AWS using Ansible automation&lt;/a&gt;.Â For information on creating instances using workflow templates, read the article, &lt;a href="https://developers.redhat.com/articles/2023/04/28/step-step-guide-creating-amazon-web-services-aws-ec2-instance-workflow-using"&gt;How to create an EC2 instance in AWS using Ansible workflow&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you want understand automation more in-depth, refer to the &lt;a href="https://developers.redhat.com/e-books/it-executives-guide-automation"&gt;IT Executive's Guide to Automation&lt;/a&gt; e-book, which provides a comprehensive overview of automation and its impact on businesses. If you're new to Ansible Automation Platform, you can &lt;a href="https://developers.redhat.com/products/ansible/download"&gt;download&lt;/a&gt; it and &lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;get started by exploring interactive labs&lt;/a&gt; at no cost.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/10/benefits-deploying-ansible-automation-platform-aws" title="The benefits of deploying Ansible Automation Platform on AWS"&gt;The benefits of deploying Ansible Automation Platform on AWS&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Deepankar Jain, Himanshu Yadav</dc:creator><dc:date>2023-08-10T07:00:00Z</dc:date></entry></feed>
